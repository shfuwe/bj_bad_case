{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "leishu=4\n",
    "device0 = torch.device('cuda:4' if torch.cuda.is_available() else \"cpu\")#训练集gpu\n",
    "device1 = torch.device('cuda:4' if torch.cuda.is_available() else \"cpu\")#测试集gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader,TensorDataset,Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text2token(text,tokenizer,max_length=100):\n",
    "    text2id = tokenizer(\n",
    "        text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids=text2id[\"input_ids\"].tolist()\n",
    "    attention_mask=text2id[\"attention_mask\"].tolist()\n",
    "    return input_ids,attention_mask\n",
    "def data2token(data_,tokenizer):\n",
    "    text=[i for i in data_['title'].values]\n",
    "    input_ids,attention_mask=text2token(text,tokenizer)\n",
    "    data_['input_ids']=input_ids\n",
    "    data_['attention_mask']=attention_mask\n",
    "    return data_\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.dataset = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"title\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        attention_mask = self.dataset.loc[idx, \"attention_mask\"]\n",
    "        sample = {\"text\": text, \"label\": label,\"input_ids\":input_ids,\"attention_mask\":attention_mask}\n",
    "        # print(sample)\n",
    "        return sample\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv(\"../data/train.csv\",header=None)\n",
    "data_val=pd.read_csv(\"../data/test.csv\",header=None)\n",
    "\n",
    "data_train.columns=['label','title','00']\n",
    "data_val.columns=['label','title','00']\n",
    "del data_train['00']\n",
    "del data_val['00']\n",
    "\n",
    "data_train_label=data_train['label'].tolist()\n",
    "data_val_label=data_val['label'].tolist()\n",
    "\n",
    "data_train_label=[i-1 for i in data_train_label]\n",
    "data_val_label=[i-1 for i in data_val_label]\n",
    "\n",
    "data_train['label']=data_train_label\n",
    "data_val['label']=data_val_label\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "data_train=data2token(data_train,tokenizer)\n",
    "data_val=data2token(data_val,tokenizer)\n",
    "\n",
    "#按batch_size分\n",
    "batch_size=32\n",
    "train_loader = DataLoader(\n",
    "    SentimentDataset(data_train), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    SentimentDataset(data_val), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model.resize_token_embeddings(len(tokenizer))##############\n",
    "        self.model.to(device)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, leishu)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "#         print(outputs[0])torch.Size([8, 100, 768])\n",
    "#         print(outputs[1])torch.Size([8, 768])\n",
    "#         print(outputs[0][:,0,:])torch.Size([8, 768])\n",
    "        x = outputs[1]\n",
    "#         x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "softmax = nn.Softmax(dim=1)\n",
    "criterion = nn.CrossEntropyLoss()#weight=weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义日志（data文件夹下，同级目录新建一个data文件夹）\n",
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "tz = pytz.timezone('Asia/Shanghai')\n",
    "def write_log(w):\n",
    "    file_name = '../data/' + datetime.date.today().strftime('%m%d') + \"_{}.log\".format(\"bert_base_ct\")\n",
    "    t0 = datetime.datetime.now(tz).strftime('%H:%M:%S')\n",
    "    info = \"{} : {}\".format(t0, w)\n",
    "    print(info)\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(info + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features,labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device=device0\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        if labels.shape[0] != batch_size:\n",
    "            raise ValueError('Num of labels does not match num of features')\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        \n",
    "        # compute mask_contrast\n",
    "        mask_contrast=1-mask\n",
    "\n",
    "        # compute mask_same\n",
    "        diag = torch.diag(mask)\n",
    "        a_diag = torch.diag_embed(diag)\n",
    "        mask_same= mask - a_diag\n",
    "\n",
    "        \n",
    "        # compute logits\n",
    "        logits = torch.div(torch.matmul(features, features.T),0.3)\n",
    "#         diagl = torch.diag(logits)\n",
    "#         logits=logits-diagl\n",
    "        \n",
    "        exp_logits=torch.exp(logits)\n",
    "#         b = torch.zeros(batch_size, batch_size).to(device)\n",
    "#         exp_logits=torch.where(exp_logits !=1, exp_logits, b)\n",
    "        \n",
    "    \n",
    "        # compute logits_contrast_sum_exp logits_same mask_same_sum\n",
    "        logits_contrast_exp = exp_logits * mask_contrast\n",
    "        logits_contrast_sum_exp=logits_contrast_exp.sum(1, keepdim=True)\n",
    "\n",
    "        logits_same=logits * mask_same\n",
    "        \n",
    "        mask_same_sum=mask_same.sum(1,keepdim=True)\n",
    "        \n",
    "        # compute mean_log_prob_pos\n",
    "        mean_log_prob_pos=(logits_same-torch.log(logits_contrast_sum_exp)).sum(1,keepdim=True)/mask_same_sum#某类只有一个mask_same_sum为0\n",
    "        \n",
    "        # compute loss\n",
    "#         loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss=-mean_log_prob_pos/100\n",
    "        zero = torch.zeros_like(loss)\n",
    "#         print(loss)\n",
    "#         loss=torch.where(loss > -99999, loss, zero)\n",
    "#         loss=torch.where(loss < 99999, loss, zero)\n",
    "#         print(loss)\n",
    "        return loss.mean()\n",
    "        \n",
    "#         sum_=torch.tensor([0.0]).to(device)\n",
    "#         for i in loss:\n",
    "#             if not math.isinf(i) and not math.isnan(i):\n",
    "#                 sum_+=i\n",
    "#         return sum_\n",
    "\n",
    "        \n",
    "        \n",
    "criterion_ct= SupConLoss()\n",
    "criterion_ct.to(device0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(device_test):\n",
    "    cls.to(device_test)\n",
    "    cls.eval()\n",
    "\n",
    "    epoch_loss=0\n",
    "    total=0\n",
    "    correct=0\n",
    "    output_all=[]\n",
    "    label_all=[]\n",
    "    for batch_idx,batch in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "#             print(batch['label'])\n",
    "            label=batch['label'].to(device_test)#batch size * 1\n",
    "            label_all.append(label.view(-1,1))\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device_test)#batch size * 100\n",
    "            attention_mask=torch.stack(batch['attention_mask']).t().to(device_test)#batch size * 100\n",
    "            \n",
    "            #计算输出\n",
    "            output = cls(input_ids, attention_mask=attention_mask)#batch size * 1\n",
    "            total+=len(output)\n",
    "            \n",
    "            #计算loss\n",
    "            \n",
    "#             print(output,label)\n",
    "            loss = criterion(output, label)\n",
    "            epoch_loss+=loss\n",
    "            ave_loss=epoch_loss/total\n",
    "            \n",
    "            #四舍五入\n",
    "            output=softmax(output)\n",
    "            output=output.argmax(dim=1)\n",
    "            output_all.append(output)\n",
    "            \n",
    "            #计算准确率\n",
    "            add_correct=(output== label).sum().item()\n",
    "            correct+=add_correct\n",
    "            acc=correct/total\n",
    "            \n",
    "            if batch_idx%5==0:\n",
    "                print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    batch_idx, len(test_loader),100.*batch_idx/len(test_loader), \n",
    "                    correct, total,acc,\n",
    "                    ave_loss\n",
    "                    ),end= \"\\r\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    #结束：\n",
    "    write_log('正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    correct, total,acc,\n",
    "                    ave_loss))\n",
    "    \n",
    "#     can't convert cuda:5 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
    "    output_all=torch.cat(output_all,0)\n",
    "    label_all=torch.cat(label_all,0)\n",
    "    \n",
    "    output_all=np.array(output_all.cpu())\n",
    "    label_all=np.array(label_all.cpu())\n",
    "    acc_score=metrics.accuracy_score(label_all,output_all)\n",
    "    write_log(metrics.classification_report(label_all,output_all))\n",
    "    write_log(\"准确率:\"+str(acc_score))\n",
    "    \n",
    "    \n",
    "    return acc,epoch_loss.item()\n",
    "\n",
    "# test(device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(min_test_epoch_loss,device_train,epoch_num):\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"_______________train epoch\"+str(epoch_num)+\" start_______________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    cls.to(device_train)\n",
    "    cls.train()\n",
    "\n",
    "    epoch_loss=0\n",
    "    total=0\n",
    "    correct=0\n",
    "    output_all=[]\n",
    "    label_all=[]\n",
    "    for batch_idx,batch in enumerate(train_loader):\n",
    "        label=batch['label'].to(device_train)#batch size * 1\n",
    "        label_all.append(label.view(-1,1))\n",
    "        input_ids=torch.stack(batch['input_ids']).t().to(device_train)#batch size * 100\n",
    "        attention_mask=torch.stack(batch['attention_mask']).t().to(device_train)#batch size * 100\n",
    "\n",
    "        #计算输出\n",
    "        output = cls(input_ids, attention_mask=attention_mask)#batch size * 1\n",
    "\n",
    "        #计算loss\n",
    "        loss = criterion(output, label)\n",
    "        loss_ct = criterion_ct(output, label)\n",
    "        if math.isinf(loss_ct) or math.isnan(loss_ct):\n",
    "            loss_all=loss\n",
    "        else:\n",
    "            loss_all = loss+loss_ct\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_all.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            #四舍五入\n",
    "            output=softmax(output)\n",
    "            output=output.argmax(dim=1)\n",
    "            output_all.append(output)\n",
    "            total+=len(output)\n",
    "            \n",
    "            #epoch_loss\n",
    "            epoch_loss+=loss\n",
    "            ave_loss=epoch_loss/total\n",
    "            \n",
    "            #计算准确率\n",
    "            add_correct=(output== label).sum().item()\n",
    "            correct+=add_correct\n",
    "            acc=correct/total\n",
    "            \n",
    "            if batch_idx%5==0:\n",
    "                print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    batch_idx, len(train_loader),100.*batch_idx/len(train_loader), \n",
    "                    correct, total,acc,\n",
    "                    ave_loss\n",
    "                    ),end= \"\\r\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    #结束：\n",
    "    \n",
    "#     can't convert cuda:5 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
    "    with torch.no_grad():\n",
    "        output_all=torch.cat(output_all,0)\n",
    "        label_all=torch.cat(label_all,0)\n",
    "\n",
    "        output_all=np.array(output_all.cpu())\n",
    "        label_all=np.array(label_all.cpu())\n",
    "        acc_score=metrics.accuracy_score(label_all,output_all)\n",
    "        \n",
    "#     print(metrics.classification_report(label_all,output_all))\n",
    "    write_log('__________________train end__________________')\n",
    "    write_log('正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    correct, total,acc,\n",
    "                    ave_loss))\n",
    "    \n",
    "    write_log(\"准确率:\"+str(acc_score))\n",
    "    \n",
    "    write_log('__________________test start__________________')\n",
    "    test_acc,test_epoch_loss=test(device1)\n",
    "    if min_test_epoch_loss>test_epoch_loss:\n",
    "        min_test_epoch_loss=test_epoch_loss\n",
    "        write_log(\"store model\")\n",
    "        end = time.time()\n",
    "        torch.save(cls,\"../data/bert/cls_\"+str(epoch_num)+\"_\"+str(round(test_acc,5))+'_'+str(round(test_epoch_loss,5))+'_'+str(end)+\".model\")\n",
    "    \n",
    "    write_log('train_acc:'+str(acc)+'  train_epoch_loss:'+str(epoch_loss.item())+'  test_acc:'+str(test_acc)+'  test_epoch_loss:'+str(test_epoch_loss))\n",
    "    write_log('__________________test end__________________')\n",
    "    \n",
    "    train_acc_l.append(acc)\n",
    "    train_epoch_loss_l.append(epoch_loss.item())\n",
    "    test_acc_l.append(test_acc)\n",
    "    test_epoch_loss_l.append(test_epoch_loss)\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"_______________train epoch \"+str(epoch_num)+\" end_______________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    return min_test_epoch_loss\n",
    "    \n",
    "\n",
    "    \n",
    "def train(epoch_num):\n",
    "    min_test_epoch_loss=999999\n",
    "    for i in range(epoch_num):\n",
    "        min_test_epoch_loss=train_one_epoch(min_test_epoch_loss,device0,i)\n",
    "\n",
    "    \n",
    "# train_one_epoch(device0,0)\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = fn_cls(device0)\n",
    "# cls=torch.load(\"../data/cls_0.86353_183.34991_1642153938.7560434.model\",map_location=device0)\n",
    "from torch import optim\n",
    "optimizer = optim.Adam(cls.parameters(), lr=1e-5)\n",
    "# test(device1)\n",
    "train_acc_l=[]\n",
    "train_epoch_loss_l=[]\n",
    "test_acc_l=[]\n",
    "test_epoch_loss_l=[]\n",
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# def _plt():\n",
    "#     plt.plot([i for i in range(len(train_acc_l))], train_acc_l)\n",
    "#     plt.title('train_acc')\n",
    "#     plt.show()\n",
    "#     plt.plot([i for i in range(len(train_epoch_loss_l))], train_epoch_loss_l)\n",
    "#     plt.title('train_epoch_loss')\n",
    "#     plt.show()\n",
    "#     plt.plot([i for i in range(len(test_acc_l))], test_acc_l)\n",
    "#     plt.title('test_acc')\n",
    "#     plt.show()\n",
    "#     plt.plot([i for i in range(len(test_epoch_loss_l))], test_epoch_loss_l)\n",
    "#     plt.title('test_epoch_loss')\n",
    "#     plt.show()\n",
    "# _plt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw0",
   "language": "python",
   "name": "sw0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
