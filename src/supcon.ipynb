{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import tensorboard_logger as tb_logger\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from util import AverageMeter\n",
    "# from util import adjust_learning_rate, warmup_learning_rate\n",
    "# from util import set_optimizer, save_model\n",
    "# from networks.resnet_big import SupConResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data loader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# added_token=['##char##']\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\",additional_special_tokens=added_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def text2token(text,tokenizer,max_length=100):\n",
    "    text2id = tokenizer(\n",
    "        text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids=text2id[\"input_ids\"].tolist()\n",
    "    attention_mask=text2id[\"attention_mask\"].tolist()\n",
    "    return input_ids,attention_mask\n",
    "def data2token(data_,tokenizer):\n",
    "    text=[i for i in data_['title'].values]\n",
    "    input_ids,attention_mask=text2token(text,tokenizer)\n",
    "    data_['input_ids']=input_ids\n",
    "    data_['attention_mask']=attention_mask\n",
    "    return data_\n",
    "from torch.utils.data import Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.dataset = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"title\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        attention_mask = self.dataset.loc[idx, \"attention_mask\"]\n",
    "        sample = {\"text\": text, \"label\": label,\"input_ids\":input_ids,\"attention_mask\":attention_mask}\n",
    "        # print(sample)\n",
    "        return sample\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_train=pd.read_csv(\"../data/data_train.csv\")\n",
    "data_val=pd.read_csv(\"../data/data_val.csv\")\n",
    "\n",
    "data_train=data2token(data_train,tokenizer)\n",
    "data_val=data2token(data_val,tokenizer)\n",
    "\n",
    "#按batch_size分\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=3\n",
    "train_loader = DataLoader(\n",
    "    SentimentDataset(data_train), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    SentimentDataset(data_val), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features,labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device=device0\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        if labels.shape[0] != batch_size:\n",
    "            raise ValueError('Num of labels does not match num of features')\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        \n",
    "        \n",
    "        mask_contrast=1-mask\n",
    "#         print('mask_contrast\\n',mask_contrast)\n",
    "        \n",
    "        diag = torch.diag(mask)\n",
    "        a_diag = torch.diag_embed(diag)\n",
    "        mask_same= mask - a_diag\n",
    "#         print('mask_same\\n',mask_same)\n",
    "        \n",
    "        # compute logits\n",
    "#         print(features.shape)\n",
    "        logits = torch.div(torch.matmul(features, features.T),self.temperature)\n",
    "#         print('logits\\n',logits)\n",
    "        \n",
    "#         logits_max, _ =torch.max(logits, dim=1, keepdim=True)\n",
    "#         logits = logits - logits_max.detach()\n",
    "#         print('logits\\n',logits)\n",
    "        diagl = torch.diag(logits)\n",
    "        logits=logits-diagl\n",
    "#         print('logits\\n',logits)\n",
    "\n",
    "        \n",
    "        exp_logits=torch.exp(logits)\n",
    "#         print('exp_logits\\n',exp_logits)\n",
    "        \n",
    "        b = torch.zeros(batch_size, batch_size).to(device)\n",
    "        exp_logits=torch.where(exp_logits !=1, exp_logits, b)\n",
    "#         print('exp_logits\\n',exp_logits)\n",
    "\n",
    "        \n",
    "        # compute log_prob\n",
    "        logits_contrast = exp_logits * mask_contrast\n",
    "#         print('logits_contrast\\n',logits_contrast)\n",
    "        logits_same=exp_logits * mask_same\n",
    "#         print('logits_same\\n',logits_same)\n",
    "        \n",
    "        logits_contrast_sum=logits_contrast.sum(1, keepdim=True)\n",
    "#         print('logits_contrast_sum\\n',logits_contrast_sum)\n",
    "        logits_same_sum=logits_same.sum(1, keepdim=True)\n",
    "#         print('logits_same_sum\\n',logits_same_sum)\n",
    "        \n",
    "#         print('torch.log(logits_same_sum)-torch.log(logits_contrast_sum)\\n',torch.log(logits_same_sum)-torch.log(logits_contrast_sum))\n",
    "        mask_same_sum=mask_same.sum(1,keepdim=True)\n",
    "#         print('mask_same.sum(1,keepdim=True)\\n',mask_same_sum)\n",
    "        mean_log_prob_pos=(torch.log(logits_same_sum)-torch.log(logits_contrast_sum))/mask_same_sum\n",
    "#         print('mean_log_prob_pos',mean_log_prob_pos)\n",
    "        \n",
    "        \n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        sum_=torch.tensor([0.0]).to(device)\n",
    "        for i in loss:\n",
    "            if not math.isinf(i) and not math.isnan(i):\n",
    "                sum_+=i\n",
    "#         print('sum_\\n',sum_)\n",
    "        if sum_==0:\n",
    "            return sum_\n",
    "        else:\n",
    "            return torch.exp(sum_)\n",
    "# criterion= SupConLoss(temperature=opt.temp)\n",
    "# criterion.to(device0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X = torch.tensor([[[0.2,0.2,0.6],[0.2,0.2,0.6]],[[0.1,0.1,0.8],[0.1,0.1,0.8]]])\n",
    "# X = torch.tensor([[0.1,0.1,0.8],[0.2,0.2,0.6],[0.3,0.3,0.4],[0.4,0.4,0.2]])\n",
    "# # X = torch.tensor([[0.1,0.1,0.8],[0.1,0.1,0.8],[0.1,0.1,0.8],[0.1,0.1,0.8]])\n",
    "# y = torch.tensor([2,2,1,2])\n",
    "# print(X.shape,X,y)\n",
    "\n",
    "# criterion(X.to(device0),y.to(device0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# build model and criterion\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device0 = torch.device('cuda:7' if torch.cuda.is_available() else \"cpu\")#训练集gpu\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model.resize_token_embeddings(len(tokenizer))##############\n",
    "        self.model.to(device)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "#         print(outputs)\n",
    "#         print(outputs[0])torch.Size([8, 100, 768])\n",
    "#         print(outputs[1])torch.Size([8, 768])\n",
    "#         print(outputs[0][:,0,:])torch.Size([8, 768])\n",
    "        x = outputs[1]\n",
    "#         x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = fn_cls(device0)\n",
    "criterion = SupConLoss()\n",
    "model.to(device0)\n",
    "criterion.to(device0)\n",
    "\n",
    "# build optimizer\n",
    "from torch import optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "# tensorboard\n",
    "# logger = tb_logger.Logger(logdir=opt.tb_folder, flush_secs=2)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "criterioncc = nn.CrossEntropyLoss().to(device0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"one epoch training\"\"\"\n",
    "    model.train()\n",
    "    label_all=[]\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    end = time.time()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        print('___________________________',str(idx),'___________________________')\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        labels=batch['label'].to(device0)#batch size * 1\n",
    "        label_all.append(labels.view(-1,1))\n",
    "        input_ids=torch.stack(batch['input_ids']).t().to(device0)#batch size * 100\n",
    "        attention_mask=torch.stack(batch['attention_mask']).t().to(device0)#batch size * 100\n",
    "\n",
    "        bsz = labels.shape[0]\n",
    "\n",
    "        # warm-up learning rate\n",
    "#         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n",
    "\n",
    "        # compute loss\n",
    "        features = model(input_ids, attention_mask=attention_mask)\n",
    "        print(features)\n",
    "        losscc=criterioncc(features,labels)\n",
    "        \n",
    "        features=softmax(features)\n",
    "        print(features)\n",
    "\n",
    "        loss = criterion(features,labels)\n",
    "        \n",
    "#         loss_a=losscc+loss\n",
    "        loss_a=losscc+loss/1000\n",
    "        # update metric\n",
    "        \n",
    "\n",
    "        # SGD\n",
    "        if loss_a!=0:\n",
    "            print('losscc\\n',losscc)\n",
    "            print('loss\\n',loss)\n",
    "            losses.update(loss_a.item(), bsz)\n",
    "            optimizer.zero_grad()\n",
    "            loss_a.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # print info\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'batch_time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'data_time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'loss {loss.val:.3f} ({loss.avg:.3f})'.format(\n",
    "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________ 0 ___________________________\n",
      "tensor([[-0.3796, -0.7810, -0.5871,  0.2546],\n",
      "        [-0.2860, -0.7125, -0.5380,  0.3207],\n",
      "        [ 0.1331, -0.3649,  0.1013, -0.5504]], device='cuda:7',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2290, 0.1533, 0.1861, 0.4317],\n",
      "        [0.2345, 0.1531, 0.1823, 0.4302],\n",
      "        [0.3245, 0.1972, 0.3144, 0.1638]], device='cuda:7',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(1.1385, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([1.6525], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "___________________________ 1 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 2 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 3 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 4 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 5 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 6 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 7 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 8 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 9 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 10 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 11 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 12 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 13 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 14 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 15 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 16 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 17 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 18 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 19 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________ 20 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 21 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 22 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 23 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 24 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 25 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 26 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 27 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 28 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 29 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 30 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 31 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 32 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 33 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 34 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 35 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 36 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 37 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 38 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 39 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 40 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 41 ___________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 42 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n",
      "___________________________ 43 ___________________________\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<AddmmBackward0>)\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:7', grad_fn=<SoftmaxBackward0>)\n",
      "losscc\n",
      " tensor(nan, device='cuda:7', grad_fn=<NllLossBackward0>)\n",
      "loss\n",
      " tensor([0.], device='cuda:7')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30920/2572273085.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch {}, total time {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30920/2590124740.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mloss_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# measure elapsed time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bs0/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bs0/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bs0/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bs0/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training routine\n",
    "for epoch in range(10):\n",
    "#     adjust_learning_rate(opt, optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    time1 = time.time()\n",
    "    loss = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    time2 = time.time()\n",
    "    print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the last model\n",
    "save_file = os.path.join(\n",
    "    opt.save_folder, 'last.pth')\n",
    "save_model(model, optimizer, opt, opt.epochs, save_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
