{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#按batch_size分\n",
    "from torch.utils.data import DataLoader,TensorDataset,Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义日志（data文件夹下，同级目录新建一个data文件夹）\n",
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "tz = pytz.timezone('Asia/Shanghai')\n",
    "def write_log(w):\n",
    "    file_name = '../data/' + datetime.date.today().strftime('%m%d') + \"_{}.log\".format(\"bert_base4\")\n",
    "    t0 = datetime.datetime.now(tz).strftime('%H:%M:%S')\n",
    "    info = \"{} : {}\".format(t0, w)\n",
    "    print(info)\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(info + '\\n')\n",
    "# write_log('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# added_token=['##char##']\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\",additional_special_tokens=added_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def text2token(text,tokenizer,max_length=100):\n",
    "    text2id = tokenizer(\n",
    "        text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids=text2id[\"input_ids\"].tolist()\n",
    "    attention_mask=text2id[\"attention_mask\"].tolist()\n",
    "    return input_ids,attention_mask\n",
    "def data2token(data_,tokenizer):\n",
    "    text=[i for i in data_['title'].values]\n",
    "    input_ids,attention_mask=text2token(text,tokenizer)\n",
    "    data_['input_ids']=input_ids\n",
    "    data_['attention_mask']=attention_mask\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.dataset = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"title\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "        pre = self.dataset.loc[idx, \"pre\"]\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        attention_mask = self.dataset.loc[idx, \"attention_mask\"]\n",
    "        sample = {\"text\": text, \"label\": label,\"pre\":pre,\"input_ids\":input_ids,\"attention_mask\":attention_mask}\n",
    "        # print(sample)\n",
    "        return sample\n",
    "    \n",
    "bad_case=pd.read_csv(\"../data/data_test1_bad.csv\")\n",
    "bad_case=data2token(bad_case,tokenizer)\n",
    "bad_loader = DataLoader(\n",
    "    SentimentDataset(bad_case), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.dataset = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"title\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        attention_mask = self.dataset.loc[idx, \"attention_mask\"]\n",
    "        sample = {\"text\": text, \"label\": label,\"input_ids\":input_ids,\"attention_mask\":attention_mask}\n",
    "        # print(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "data_test1=pd.read_csv(\"../data/data_test1.csv\")\n",
    "data_test1=data2token(data_test1,tokenizer)\n",
    "test1_loader = DataLoader(\n",
    "    SentimentDataset(data_test1), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "data_test2=pd.read_csv(\"../data/data_test2.csv\")\n",
    "data_test2=data2token(data_test2,tokenizer)\n",
    "test2_loader = DataLoader(\n",
    "    SentimentDataset(data_test2), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "data_train=pd.read_csv(\"../data/data_train.csv\")\n",
    "data_train=data2token(data_train,tokenizer)\n",
    "batch_size=16\n",
    "train_loader = DataLoader(\n",
    "    SentimentDataset(data_train), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "data_val=pd.read_csv(\"../data/data_val.csv\")\n",
    "data_val=data2token(data_val,tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    SentimentDataset(data_val), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "def predict_loader(device,test_loader,cls):\n",
    "    with torch.no_grad():\n",
    "        cls.to(device)\n",
    "        cls.eval()\n",
    "        output_all=[]\n",
    "        label_all=[]\n",
    "        for batch_idx,batch in enumerate(test_loader):\n",
    "            print(str(batch_idx)+'/'+str(len(test_loader)),end='\\r')\n",
    "            label=batch['label'].to(device)#batch size * 1\n",
    "            label_all.append(label.view(-1,1))\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device)#batch size * 100\n",
    "            attention_mask=torch.stack(batch['attention_mask']).t().to(device)#batch size * 100\n",
    "            \n",
    "            #计算输出\n",
    "            output = cls(input_ids, attention_mask=attention_mask)#batch size * 1\n",
    "            \n",
    "            #四舍五入\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            output=softmax(output)\n",
    "            output=output.argmax(dim=1)\n",
    "            output_all.append(output)\n",
    "        output_all=torch.cat(output_all,0)\n",
    "        label_all=torch.cat(label_all,0)\n",
    "\n",
    "        output_all=np.array(output_all.cpu())\n",
    "        label_all=np.array(label_all.cpu())\n",
    "        acc_score=metrics.accuracy_score(label_all,output_all)\n",
    "        print(\"准确率:\"+str(acc_score))\n",
    "        print(metrics.classification_report(label_all,output_all))\n",
    "        return label_all,output_all\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model.resize_token_embeddings(len(tokenizer))##############\n",
    "        self.model.to(device)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "#         print(outputs[0])torch.Size([8, 100, 768])\n",
    "#         print(outputs[1])torch.Size([8, 768])\n",
    "#         print(outputs[0][:,0,:])torch.Size([8, 768])\n",
    "        x = outputs[1]\n",
    "#         x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "import torch\n",
    "device0 = torch.device('cuda:7' if torch.cuda.is_available() else \"cpu\")#训练集gpu\n",
    "softmax = nn.Softmax(dim=1)\n",
    "criterion = nn.CrossEntropyLoss()#weight=weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model1对每个badcase预测，然后修正model2\n",
    "def train_one_bad_epoch(device_train,lmd=10):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    cls2.to(device_train)\n",
    "    \n",
    "    epoch_loss=0\n",
    "    total=0\n",
    "    correct=0\n",
    "    output_all=[]\n",
    "    label_all=[]\n",
    "    for batch_idx,batch in enumerate(bad_loader):\n",
    "#         print('___________batch'+str(batch_idx)+'___________')\n",
    "        label=batch['label'].to(device_train)#batch size * 1\n",
    "        pre=batch['pre'].to(device_train)\n",
    "        input_ids=torch.stack(batch['input_ids']).t().to(device_train)#batch size * 100\n",
    "        attention_mask=torch.stack(batch['attention_mask']).t().to(device_train)#batch size * 100\n",
    "                \n",
    "        output2=cls2(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(output2, label)\n",
    "        \n",
    "        optimizer2.zero_grad() # 将所有参数的梯度都置零\n",
    "        loss.backward()    # 误差反向传播计算参数梯度\n",
    "        optimizer2.step()    # 通过梯度做一步参数更新\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            total+=len(output2)\n",
    "            epoch_loss+=loss.item()\n",
    "            output2=softmax(output2)\n",
    "            output2=output2.argmax(dim=1)\n",
    "            add_correct=(output2== pre).sum().item()\n",
    "            correct+=add_correct\n",
    "            acc=correct/total\n",
    "        print(str(batch_idx)+'/'+str(len(bad_loader))+'\\tbatch_loss:'+str(loss.item())+'\\tbad_acc:'+str(acc),end='\\r')\n",
    "        \n",
    "        \n",
    "    return epoch_loss,acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(device_test):\n",
    "    cls2.to(device_test)\n",
    "    cls2.eval()\n",
    "\n",
    "    epoch_loss=0\n",
    "    total=0\n",
    "    correct=0\n",
    "    output_all=[]\n",
    "    label_all=[]\n",
    "    for batch_idx,batch in enumerate(test1_loader):\n",
    "        with torch.no_grad():\n",
    "#             print(batch['label'])\n",
    "            label=batch['label'].to(device_test)#batch size * 1\n",
    "            label_all.append(label.view(-1,1))\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device_test)#batch size * 100\n",
    "            attention_mask=torch.stack(batch['attention_mask']).t().to(device_test)#batch size * 100\n",
    "            \n",
    "            #计算输出\n",
    "            output = cls2(input_ids, attention_mask=attention_mask)#batch size * 1\n",
    "            total+=len(output)\n",
    "            \n",
    "            #计算loss\n",
    "            \n",
    "#             print(output,label)\n",
    "            loss = criterion(output, label)\n",
    "            epoch_loss+=loss\n",
    "            ave_loss=epoch_loss/total\n",
    "            \n",
    "            #四舍五入\n",
    "            output=softmax(output)\n",
    "            output=output.argmax(dim=1)\n",
    "            output_all.append(output)\n",
    "            \n",
    "            #计算准确率\n",
    "            add_correct=(output== label).sum().item()\n",
    "            correct+=add_correct\n",
    "            acc=correct/total\n",
    "            \n",
    "            if batch_idx%5==0:\n",
    "                print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    batch_idx, len(test1_loader),100.*batch_idx/len(test1_loader), \n",
    "                    correct, total,acc,\n",
    "                    ave_loss\n",
    "                    ),end= \"\\r\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    #结束：\n",
    "    write_log('正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    correct, total,acc,\n",
    "                    ave_loss))\n",
    "    \n",
    "#     can't convert cuda:5 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
    "    output_all=torch.cat(output_all,0)\n",
    "    label_all=torch.cat(label_all,0)\n",
    "    \n",
    "    output_all=np.array(output_all.cpu())\n",
    "    label_all=np.array(label_all.cpu())\n",
    "    acc_score=metrics.accuracy_score(label_all,output_all)\n",
    "    print(metrics.classification_report(label_all,output_all))\n",
    "    write_log(\"准确率:\"+str(acc_score))\n",
    "    \n",
    "    return acc,epoch_loss.item()\n",
    "\n",
    "# test(device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(min_test_epoch_loss,device_train,epoch_num,lmd=10):\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"_______________train epoch\"+str(epoch_num)+\" start_______________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    cls2.to(device_train)\n",
    "    cls2.train()\n",
    "\n",
    "    epoch_loss=0\n",
    "    total=0\n",
    "    correct=0\n",
    "    acc=0\n",
    "    ave_loss=0\n",
    "    output_all=[]\n",
    "    label_all=[]\n",
    "    for batch_idx,batch in enumerate(train_loader):\n",
    "        if batch_idx%500==0:\n",
    "            write_log('正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    correct, total,acc,\n",
    "                    ave_loss))\n",
    "            write_log(\"___train_one_bad_epoch___\")\n",
    "            epoch_loss,acc=train_one_bad_epoch(device0,lmd)\n",
    "            write_log(\"bad_epoch_loss:\"+str(epoch_loss)+\"\\tbad_acc:\"+str(acc))\n",
    "            \n",
    "        label=batch['label'].to(device_train)#batch size * 1\n",
    "        label_all.append(label.view(-1,1))\n",
    "        input_ids=torch.stack(batch['input_ids']).t().to(device_train)#batch size * 100\n",
    "        attention_mask=torch.stack(batch['attention_mask']).t().to(device_train)#batch size * 100\n",
    "\n",
    "        #计算输出\n",
    "        output = cls2(input_ids, attention_mask=attention_mask)#batch size * 1\n",
    "\n",
    "        #计算loss\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        optimizer2.zero_grad()\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #四舍五入\n",
    "            output=softmax(output)\n",
    "            output=output.argmax(dim=1)\n",
    "            output_all.append(output)\n",
    "            total+=len(output)\n",
    "            \n",
    "            #epoch_loss\n",
    "            epoch_loss+=loss\n",
    "            ave_loss=epoch_loss/total\n",
    "            \n",
    "            #计算准确率\n",
    "            add_correct=(output== label).sum().item()\n",
    "            correct+=add_correct\n",
    "            acc=correct/total\n",
    "            \n",
    "            if batch_idx%5==0:\n",
    "                print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    batch_idx, len(train_loader),100.*batch_idx/len(train_loader), \n",
    "                    correct, total,acc,\n",
    "                    ave_loss\n",
    "                    ),end= \"\\r\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    #结束：\n",
    "    \n",
    "#     can't convert cuda:5 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
    "    with torch.no_grad():\n",
    "        output_all=torch.cat(output_all,0)\n",
    "        label_all=torch.cat(label_all,0)\n",
    "\n",
    "        output_all=np.array(output_all.cpu())\n",
    "        label_all=np.array(label_all.cpu())\n",
    "        acc_score=metrics.accuracy_score(label_all,output_all)\n",
    "        \n",
    "#     print(metrics.classification_report(label_all,output_all))\n",
    "    write_log('__________________train end__________________')\n",
    "    write_log('正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    correct, total,acc,\n",
    "                    ave_loss))\n",
    "    \n",
    "    write_log(\"准确率:\"+str(acc_score))\n",
    "    \n",
    "    write_log('__________________test start__________________')\n",
    "    test_acc,test_epoch_loss=test(device0)\n",
    "    if min_test_epoch_loss>test_epoch_loss:\n",
    "        min_test_epoch_loss=test_epoch_loss\n",
    "        write_log(\"store model\")\n",
    "        end = time.time()\n",
    "        torch.save(cls2,\"../data/cls_bad_\"+str(epoch_num)+'_'+str(round(test_acc,5))+'_'+str(round(test_epoch_loss,5))+\".model\")\n",
    "    \n",
    "    write_log('__________________test end__________________')\n",
    "    write_log('train_acc:'+str(acc)+'  train_epoch_loss:'+str(epoch_loss.item())+'  test_acc:'+str(test_acc)+'  test_epoch_loss:'+str(test_epoch_loss))\n",
    "    \n",
    "    \n",
    "    train_acc_l.append(acc)\n",
    "    train_epoch_loss_l.append(epoch_loss.item())\n",
    "    test_acc_l.append(test_acc)\n",
    "    test_epoch_loss_l.append(test_epoch_loss)\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"_______________train epoch \"+str(epoch_num)+\" end_______________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    return min_test_epoch_loss\n",
    "    \n",
    "\n",
    "    \n",
    "def train(epoch_num):\n",
    "    min_test_epoch_loss=999999\n",
    "    for i in range(epoch_num):\n",
    "        min_test_epoch_loss=train_one_epoch(min_test_epoch_loss,device0,i)\n",
    "\n",
    "    \n",
    "# train_one_epoch(device0,0)\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls=torch.load(\"../data/cls_6_0.88785_266.58456.model\",map_location=device0)\n",
    "cls2=torch.load(\"../data/cls_6_0.88785_266.58456.model\",map_location=device0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:06:37 : ______________________________________________\n",
      "19:06:37 : ______________________________________________\n",
      "19:06:37 : _______________train epoch0 start_______________\n",
      "19:06:37 : ______________________________________________\n",
      "19:06:37 : ______________________________________________\n",
      "19:06:37 : 正确分类的样本数：0，样本总数：0，准确率：0，ave_loss：0\n",
      "19:06:37 : ___train_one_bad_epoch___\n",
      "19:06:58 : bad_epoch_loss:302.418319940567\tbad_acc:0.7801890094504725\n",
      "19:07:56 : 正确分类的样本数：7286，样本总数：8000，准确率：0.91075，ave_loss：0.05723505094647407575144700706005175\n",
      "19:07:56 : ___train_one_bad_epoch___\n",
      "19:08:18 : bad_epoch_loss:267.3679175376892\tbad_acc:0.7353867693384669\n",
      "19:09:16 : 正确分类的样本数：14543，样本总数：16000，准确率：0.9089375，ave_loss：0.026742506772279746790698990225792\n",
      "19:09:16 : ___train_one_bad_epoch___\n",
      "19:09:38 : bad_epoch_loss:263.371554851532\tbad_acc:0.744137206860343\n",
      "19:10:37 : 正确分类的样本数：21798，样本总数：24000，准确率：0.90825，ave_loss：0.017497098073363304017506269738078117\n",
      "19:10:37 : ___train_one_bad_epoch___\n",
      "19:10:58 : bad_epoch_loss:256.71432185173035\tbad_acc:0.7101855092754638\n",
      "19:11:57 : 正确分类的样本数：29036，样本总数：32000，准确率：0.907375，ave_loss：0.01299430150538683012990349903702736\n",
      "19:11:57 : ___train_one_bad_epoch___\n",
      "19:12:18 : bad_epoch_loss:251.20250988006592\tbad_acc:0.7021351067553377\n",
      "19:13:17 : 正确分类的样本数：36268，样本总数：40000，准确率：0.9067，ave_loss：0.010266733355820179.010257450863718987\n",
      "19:13:17 : ___train_one_bad_epoch___\n",
      "19:13:39 : bad_epoch_loss:243.95108807086945\tbad_acc:0.6790339516975848\n",
      "19:14:38 : 正确分类的样本数：43554，样本总数：48000，准确率：0.907375，ave_loss：0.008318034932017326083088595420122155\n",
      "19:14:38 : ___train_one_bad_epoch___\n",
      "19:14:59 : bad_epoch_loss:237.3707219362259\tbad_acc:0.6583829191459573\n",
      "19:15:58 : 正确分类的样本数：50800，样本总数：56000，准确率：0.9071428571428571，ave_loss：0.00698724389076232925881136\n",
      "19:15:58 : ___train_one_bad_epoch___\n",
      "19:16:19 : bad_epoch_loss:234.26911360025406\tbad_acc:0.6398319915995799\n",
      "19:17:17 : __________________train end__________________03963873558，ave_loss：0.0060590077191591265\n",
      "19:17:17 : 正确分类的样本数：57849，样本总数：63800，准确率：0.9067241379310345，ave_loss：0.006062098313122988\n",
      "19:17:17 : 准确率:0.9067241379310345\n",
      "19:17:17 : __________________test start__________________\n",
      "19:18:20 : 正确分类的样本数：23178，样本总数：25520，准确率：0.9082288401253918，ave_loss：0.0165149774402383289961815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.92      6380\n",
      "           1       0.95      0.96      0.95      6380\n",
      "           2       0.89      0.85      0.87      6380\n",
      "           3       0.86      0.93      0.89      6380\n",
      "\n",
      "    accuracy                           0.91     25520\n",
      "   macro avg       0.91      0.91      0.91     25520\n",
      "weighted avg       0.91      0.91      0.91     25520\n",
      "\n",
      "19:18:20 : 准确率:0.9082288401253918\n",
      "19:18:20 : store model\n",
      "19:18:21 : __________________test end__________________\n",
      "19:18:21 : train_acc:0.9067241379310345  train_epoch_loss:386.7618713378906  test_acc:0.9082288401253918  test_epoch_loss:421.46221923828125\n",
      "19:18:21 : ______________________________________________\n",
      "19:18:21 : ______________________________________________\n",
      "19:18:21 : _______________train epoch 0 end_______________\n",
      "19:18:21 : ______________________________________________\n",
      "19:18:21 : ______________________________________________\n",
      "19:18:21 : ______________________________________________\n",
      "19:18:21 : ______________________________________________\n",
      "19:18:21 : _______________train epoch1 start_______________\n",
      "19:18:21 : ______________________________________________\n",
      "19:18:21 : ______________________________________________\n",
      "19:18:21 : 正确分类的样本数：0，样本总数：0，准确率：0，ave_loss：0\n",
      "19:18:21 : ___train_one_bad_epoch___\n",
      "19:18:42 : bad_epoch_loss:224.8915895819664\tbad_acc:0.6146307315365769\n",
      "19:19:41 : 正确分类的样本数：7300，样本总数：8000，准确率：0.9125，ave_loss：0.04586150869727135.046053521335124974\n",
      "19:19:41 : ___train_one_bad_epoch___\n",
      "19:20:02 : bad_epoch_loss:223.71839904785156\tbad_acc:0.6030801540077004\n",
      "19:21:01 : 正确分类的样本数：14575，样本总数：16000，准确率：0.9109375，ave_loss：0.023156981915235523188225924968726\n",
      "19:21:01 : ___train_one_bad_epoch___\n",
      "19:21:23 : bad_epoch_loss:217.4934607744217\tbad_acc:0.5855792789639482\n",
      "19:22:22 : 正确分类的样本数：21832，样本总数：24000，准确率：0.9096666666666666，ave_loss：0.0151393087580800068180695\n",
      "19:22:22 : ___train_one_bad_epoch___\n",
      "19:22:43 : bad_epoch_loss:209.18341517448425\tbad_acc:0.5554777738886945\n",
      "19:23:42 : 正确分类的样本数：29081，样本总数：32000，准确率：0.90878125，ave_loss：0.011077709496021270546257346868522\n",
      "19:23:42 : ___train_one_bad_epoch___\n",
      "19:24:03 : bad_epoch_loss:201.13844084739685\tbad_acc:0.5337766888344417\n",
      "19:25:02 : 正确分类的样本数：36340，样本总数：40000，准确率：0.9085，ave_loss：0.008690408430993557.0086833136156201365\n",
      "19:25:02 : ___train_one_bad_epoch___\n",
      "19:25:24 : bad_epoch_loss:197.58518517017365\tbad_acc:0.5127756387819391\n",
      "19:26:23 : 正确分类的样本数：43567，样本总数：48000，准确率：0.9076458333333334，ave_loss：0.00717564206570386972224665\n",
      "19:26:23 : ___train_one_bad_epoch___\n",
      "19:26:44 : bad_epoch_loss:190.13130313158035\tbad_acc:0.4914245712285614\n",
      "19:27:43 : 正确分类的样本数：50797，样本总数：56000，准确率：0.9070892857142857，ave_loss：0.00597421033307914596459866\n",
      "19:27:43 : ___train_one_bad_epoch___\n",
      "19:28:04 : bad_epoch_loss:184.66143342852592\tbad_acc:0.47007350367518375\n",
      "19:29:02 : __________________train end__________________26342197692，ave_loss：0.005002618767321117\n",
      "19:29:02 : 正确分类的样本数：57864，样本总数：63800，准确率：0.9069592476489028，ave_loss：0.005011144559830427\n",
      "19:29:02 : 准确率:0.9069592476489028\n",
      "19:29:02 : __________________test start__________________\n",
      "19:30:05 : 正确分类的样本数：23693，样本总数：25520，准确率：0.928409090909091，ave_loss：0.013223226182162762494987488\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93      6380\n",
      "           1       0.96      0.98      0.97      6380\n",
      "           2       0.90      0.90      0.90      6380\n",
      "           3       0.90      0.93      0.91      6380\n",
      "\n",
      "    accuracy                           0.93     25520\n",
      "   macro avg       0.93      0.93      0.93     25520\n",
      "weighted avg       0.93      0.93      0.93     25520\n",
      "\n",
      "19:30:05 : 准确率:0.928409090909091\n",
      "19:30:05 : store model\n",
      "19:30:06 : __________________test end__________________\n",
      "19:30:06 : train_acc:0.9069592476489028  train_epoch_loss:319.7110290527344  test_acc:0.928409090909091  test_epoch_loss:337.45672607421875\n",
      "19:30:06 : ______________________________________________\n",
      "19:30:06 : ______________________________________________\n",
      "19:30:06 : _______________train epoch 1 end_______________\n",
      "19:30:06 : ______________________________________________\n",
      "19:30:06 : ______________________________________________\n",
      "19:30:06 : ______________________________________________\n",
      "19:30:06 : ______________________________________________\n",
      "19:30:06 : _______________train epoch2 start_______________\n",
      "19:30:06 : ______________________________________________\n",
      "19:30:06 : ______________________________________________\n",
      "19:30:06 : 正确分类的样本数：0，样本总数：0，准确率：0，ave_loss：0\n",
      "19:30:06 : ___train_one_bad_epoch___\n",
      "19:30:27 : bad_epoch_loss:177.24559438228607\tbad_acc:0.4459222961148057\n",
      "19:31:26 : 正确分类的样本数：7239，样本总数：8000，准确率：0.904875，ave_loss：0.0397693514823913639980594068765644\n",
      "19:31:26 : ___train_one_bad_epoch___\n",
      "19:31:48 : bad_epoch_loss:171.48008751869202\tbad_acc:0.4294714735736787\n",
      "19:32:47 : 正确分类的样本数：14526，样本总数：16000，准确率：0.907875，ave_loss：0.0190125759691：0.019043585285544395\n",
      "19:32:47 : ___train_one_bad_epoch___\n",
      "19:33:08 : bad_epoch_loss:167.34586149454117\tbad_acc:0.4133706685334267\n",
      "19:34:07 : 正确分类的样本数：21785，样本总数：24000，准确率：0.9077083333333333，ave_loss：0.0125586912035942082107258\n",
      "19:34:07 : ___train_one_bad_epoch___\n",
      "19:34:28 : bad_epoch_loss:156.76414114236832\tbad_acc:0.39726986349317467\n",
      "19:35:27 : 正确分类的样本数：29082，样本总数：32000，准确率：0.9088125，ave_loss：0.00906949117779731805532296746969245\n",
      "19:35:27 : ___train_one_bad_epoch___\n",
      "19:35:49 : bad_epoch_loss:155.9364058971405\tbad_acc:0.37311865593279664\n",
      "19:36:48 : 正确分类的样本数：36360，样本总数：40000，准确率：0.909，ave_loss：0.0072906287387013435.0072891698218882085\n",
      "19:36:48 : ___train_one_bad_epoch___\n",
      "19:37:09 : bad_epoch_loss:146.77079284191132\tbad_acc:0.35421771088554427\n",
      "19:38:08 : 正确分类的样本数：43650，样本总数：48000，准确率：0.909375，ave_loss：0.005752154160290956557384264655411245\n",
      "19:38:08 : ___train_one_bad_epoch___\n",
      "19:38:29 : bad_epoch_loss:139.93730175495148\tbad_acc:0.3328666433321666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:39:28 : 正确分类的样本数：50906，样本总数：56000，准确率：0.9090357142857143，ave_loss：0.00489850249141454752803745\n",
      "19:39:28 : ___train_one_bad_epoch___\n",
      "19:39:50 : bad_epoch_loss:136.6386877298355\tbad_acc:0.32621631081554076\n",
      "19:40:47 : __________________train end__________________26191670848，ave_loss：0.004112833645194769\n",
      "19:40:47 : 正确分类的样本数：57992，样本总数：63800，准确率：0.9089655172413793，ave_loss：0.0041182334534823895\n",
      "19:40:47 : 准确率:0.9089655172413793\n",
      "19:40:47 : __________________test start__________________\n",
      "19:41:51 : 正确分类的样本数：23976，样本总数：25520，准确率：0.9394984326018809，ave_loss：0.01094764191657304830785275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95      6380\n",
      "           1       0.98      0.98      0.98      6380\n",
      "           2       0.93      0.88      0.91      6380\n",
      "           3       0.89      0.96      0.93      6380\n",
      "\n",
      "    accuracy                           0.94     25520\n",
      "   macro avg       0.94      0.94      0.94     25520\n",
      "weighted avg       0.94      0.94      0.94     25520\n",
      "\n",
      "19:41:51 : 准确率:0.9394984326018809\n",
      "19:41:51 : store model\n",
      "19:41:52 : __________________test end__________________\n",
      "19:41:52 : train_acc:0.9089655172413793  train_epoch_loss:262.7433166503906  test_acc:0.9394984326018809  test_epoch_loss:279.3838195800781\n",
      "19:41:52 : ______________________________________________\n",
      "19:41:52 : ______________________________________________\n",
      "19:41:52 : _______________train epoch 2 end_______________\n",
      "19:41:52 : ______________________________________________\n",
      "19:41:52 : ______________________________________________\n",
      "19:41:52 : ______________________________________________\n",
      "19:41:52 : ______________________________________________\n",
      "19:41:52 : _______________train epoch3 start_______________\n",
      "19:41:52 : ______________________________________________\n",
      "19:41:52 : ______________________________________________\n",
      "19:41:52 : 正确分类的样本数：0，样本总数：0，准确率：0，ave_loss：0\n",
      "19:41:52 : ___train_one_bad_epoch___\n",
      "19:43:12 : 正确分类的样本数：7288，样本总数：8000，准确率：0.911，ave_loss：0.0323823019862175：0.032566510140895844\n",
      "19:43:12 : ___train_one_bad_epoch___\n",
      "19:43:33 : bad_epoch_loss:125.04331371188164\tbad_acc:0.2996149807490375\n",
      "19:44:32 : 正确分类的样本数：14574，样本总数：16000，准确率：0.910875，ave_loss：0.0154606224969029435475122258067131\n",
      "19:44:32 : ___train_one_bad_epoch___\n",
      "19:44:53 : bad_epoch_loss:121.28461366891861\tbad_acc:0.28491424571228563\n",
      "19:45:52 : 正确分类的样本数：21855，样本总数：24000，准确率：0.910625，ave_loss：0.01024970971047878310220415890216827\n",
      "19:45:52 : ___train_one_bad_epoch___\n",
      "19:46:14 : bad_epoch_loss:114.2138201892376\tbad_acc:0.2621631081554078\n",
      "19:47:13 : 正确分类的样本数：29085，样本总数：32000，准确率：0.90890625，ave_loss：0.0076860198751091966794498600065715\n",
      "19:47:13 : ___train_one_bad_epoch___\n",
      "19:47:34 : bad_epoch_loss:107.88477462530136\tbad_acc:0.2408120406020301\n",
      "19:48:33 : 正确分类的样本数：36351，样本总数：40000，准确率：0.908775，ave_loss：0.005911877844482660058926721103489475\n",
      "19:48:33 : ___train_one_bad_epoch___\n",
      "19:48:54 : bad_epoch_loss:105.42885026335716\tbad_acc:0.23661183059152957\n",
      "19:49:53 : 正确分类的样本数：43653，样本总数：48000，准确率：0.9094375，ave_loss：0.00484841736033558858313559964299264\n",
      "19:49:53 : ___train_one_bad_epoch___\n",
      "19:50:14 : bad_epoch_loss:101.98425567150116\tbad_acc:0.22226111305565277\n",
      "19:51:13 : 正确分类的样本数：50924，样本总数：56000，准确率：0.9093571428571429，ave_loss：0.00408266764134168638618374\n",
      "19:51:13 : ___train_one_bad_epoch___\n",
      "19:51:34 : bad_epoch_loss:95.23184187710285\tbad_acc:0.20931046552327617\n",
      "19:52:32 : __________________train end__________________16557952835，ave_loss：0.0033757803030312067\n",
      "19:52:32 : 正确分类的样本数：58021，样本总数：63800，准确率：0.9094200626959248，ave_loss：0.003381609683856368\n",
      "19:52:32 : 准确率:0.9094200626959248\n",
      "19:52:32 : __________________test start__________________\n",
      "19:53:36 : 正确分类的样本数：24228，样本总数：25520，准确率：0.9493730407523511，ave_loss：0.00923799909651279480415344\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      6380\n",
      "           1       0.98      0.97      0.98      6380\n",
      "           2       0.93      0.92      0.93      6380\n",
      "           3       0.92      0.96      0.94      6380\n",
      "\n",
      "    accuracy                           0.95     25520\n",
      "   macro avg       0.95      0.95      0.95     25520\n",
      "weighted avg       0.95      0.95      0.95     25520\n",
      "\n",
      "19:53:36 : 准确率:0.9493730407523511\n",
      "19:53:36 : store model\n",
      "19:53:37 : __________________test end__________________\n",
      "19:53:37 : train_acc:0.9094200626959248  train_epoch_loss:215.7467041015625  test_acc:0.9493730407523511  test_epoch_loss:235.75372314453125\n",
      "19:53:37 : ______________________________________________\n",
      "19:53:37 : ______________________________________________\n",
      "19:53:37 : _______________train epoch 3 end_______________\n",
      "19:53:37 : ______________________________________________\n",
      "19:53:37 : ______________________________________________\n",
      "19:53:37 : ______________________________________________\n",
      "19:53:37 : ______________________________________________\n",
      "19:53:37 : _______________train epoch4 start_______________\n",
      "19:53:37 : ______________________________________________\n",
      "19:53:37 : ______________________________________________\n",
      "19:53:37 : 正确分类的样本数：0，样本总数：0，准确率：0，ave_loss：0\n",
      "19:53:37 : ___train_one_bad_epoch___\n",
      "19:53:58 : bad_epoch_loss:89.93327207863331\tbad_acc:0.18655932796639832\n",
      "19:54:57 : 正确分类的样本数：7311，样本总数：8000，准确率：0.913875，ave_loss：0.0264227688312530526427464559674263\n",
      "19:54:57 : ___train_one_bad_epoch___\n",
      "19:55:18 : bad_epoch_loss:85.45173367857933\tbad_acc:0.18340917045852292\n",
      "19:56:17 : 正确分类的样本数：14575，样本总数：16000，准确率：0.9109375，ave_loss：0.013211701065301895218271546065807\n",
      "19:56:17 : ___train_one_bad_epoch___\n",
      "19:56:38 : bad_epoch_loss:83.55465988814831\tbad_acc:0.17465873293664683\n",
      "19:57:37 : 正确分类的样本数：21893，样本总数：24000，准确率：0.9122083333333333，ave_loss：0.00833737570792436601636314\n",
      "19:57:37 : ___train_one_bad_epoch___\n",
      "19:57:58 : bad_epoch_loss:80.87903834879398\tbad_acc:0.1697584879243962\n",
      "19:58:57 : 正确分类的样本数：29176，样本总数：32000，准确率：0.91175，ave_loss：0.00632499111816287.0063135088421404365\n",
      "19:58:57 : ___train_one_bad_epoch___\n",
      "19:59:19 : bad_epoch_loss:75.75260008871555\tbad_acc:0.1550577528876444\n",
      "20:00:18 : 正确分类的样本数：36504，样本总数：40000，准确率：0.9126，ave_loss：0.00484153861179947850048138224519789225\n",
      "20:00:18 : ___train_one_bad_epoch___\n",
      "20:00:39 : bad_epoch_loss:74.39783611893654\tbad_acc:0.14490724536226812\n",
      "20:01:38 : 正确分类的样本数：43784，样本总数：48000，准确率：0.9121666666666667，ave_loss：0.00410788273438811387624645\n",
      "20:01:38 : ___train_one_bad_epoch___\n",
      "20:01:59 : bad_epoch_loss:71.49178171157837\tbad_acc:0.14735736786839343\n",
      "20:02:58 : 正确分类的样本数：51102，样本总数：56000，准确率：0.9125357142857143，ave_loss：0.00335556035861372956374645\n",
      "20:02:58 : ___train_one_bad_epoch___\n",
      "20:03:19 : bad_epoch_loss:68.01864103972912\tbad_acc:0.13860693034651733\n",
      "20:04:17 : __________________train end__________________81133968891，ave_loss：0.0028853053227066994\n",
      "20:04:17 : 正确分类的样本数：58206，样本总数：63800，准确率：0.912319749216301，ave_loss：0.0028891991823911667\n",
      "20:04:17 : 准确率:0.912319749216301\n",
      "20:04:17 : __________________test start__________________\n",
      "20:05:20 : 正确分类的样本数：24322，样本总数：25520，准确率：0.9530564263322884，ave_loss：0.00801474228501319971334839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      6380\n",
      "           1       0.99      0.97      0.98      6380\n",
      "           2       0.95      0.91      0.93      6380\n",
      "           3       0.92      0.97      0.94      6380\n",
      "\n",
      "    accuracy                           0.95     25520\n",
      "   macro avg       0.95      0.95      0.95     25520\n",
      "weighted avg       0.95      0.95      0.95     25520\n",
      "\n",
      "20:05:20 : 准确率:0.9530564263322884\n",
      "20:05:20 : store model\n",
      "20:05:21 : __________________test end__________________\n",
      "20:05:21 : train_acc:0.912319749216301  train_epoch_loss:184.33091735839844  test_acc:0.9530564263322884  test_epoch_loss:204.53622436523438\n",
      "20:05:21 : ______________________________________________\n",
      "20:05:21 : ______________________________________________\n",
      "20:05:21 : _______________train epoch 4 end_______________\n",
      "20:05:21 : ______________________________________________\n",
      "20:05:21 : ______________________________________________\n",
      "20:05:21 : ______________________________________________\n",
      "20:05:21 : ______________________________________________\n",
      "20:05:21 : _______________train epoch5 start_______________\n",
      "20:05:21 : ______________________________________________\n",
      "20:05:21 : ______________________________________________\n",
      "20:05:21 : 正确分类的样本数：0，样本总数：0，准确率：0，ave_loss：0\n",
      "20:05:21 : ___train_one_bad_epoch___\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:05:42 : bad_epoch_loss:65.12850910425186\tbad_acc:0.12670633531676584\n",
      "20:06:41 : 正确分类的样本数：7302，样本总数：8000，准确率：0.91275，ave_loss：0.02242578752338886322529264912009245\n",
      "20:06:41 : ___train_one_bad_epoch___\n",
      "20:07:03 : bad_epoch_loss:61.805416725575924\tbad_acc:0.11375568778438921\n",
      "20:08:02 : 正确分类的样本数：14656，样本总数：16000，准确率：0.916，ave_loss：0.010778668336570263.010750391520559788\n",
      "20:08:02 : ___train_one_bad_epoch___\n",
      "20:08:23 : bad_epoch_loss:57.19786935299635\tbad_acc:0.1064053202660133\n",
      "20:09:22 : 正确分类的样本数：22006，样本总数：24000，准确率：0.9169166666666667，ave_loss：0.00703951017931103761427315\n",
      "20:09:22 : ___train_one_bad_epoch___\n",
      "20:09:43 : bad_epoch_loss:55.73668780177832\tbad_acc:0.10045502275113756\n",
      "20:10:42 : 正确分类的样本数：29354，样本总数：32000，准确率：0.9173125，ave_loss：0.00531288795173168253048226982355125\n",
      "20:10:42 : ___train_one_bad_epoch___\n",
      "20:11:03 : bad_epoch_loss:53.36571729183197\tbad_acc:0.0983549177458873\n",
      "20:12:02 : 正确分类的样本数：36698，样本总数：40000，准确率：0.91745，ave_loss：0.0041558272205293180041427859105169775\n",
      "20:12:02 : ___train_one_bad_epoch___\n",
      "20:12:24 : bad_epoch_loss:51.42559865862131\tbad_acc:0.08750437521876094\n",
      "20:13:23 : 正确分类的样本数：44035，样本总数：48000，准确率：0.9173958333333333，ave_loss：0.00352200795896351348137174\n",
      "20:13:23 : ___train_one_bad_epoch___\n",
      "20:13:44 : bad_epoch_loss:49.755655504763126\tbad_acc:0.08890444522226111\n",
      "20:14:43 : 正确分类的样本数：51404，样本总数：56000，准确率：0.9179285714285714，ave_loss：0.00287743844091892244737196\n",
      "20:14:43 : ___train_one_bad_epoch___\n",
      "20:15:04 : bad_epoch_loss:47.21237821131945\tbad_acc:0.08365418270913545\n",
      "20:16:02 : __________________train end__________________04766683392，ave_loss：0.0025557405315339565\n",
      "20:16:02 : 正确分类的样本数：58511，样本总数：63800，准确率：0.9171003134796238，ave_loss：0.002561683766543865\n",
      "20:16:02 : 准确率:0.9171003134796238\n",
      "20:16:02 : __________________test start__________________\n",
      "20:17:05 : 正确分类的样本数：24434，样本总数：25520，准确率：0.9574451410658307，ave_loss：0.00752528756856918361578185\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96      6380\n",
      "           1       0.99      0.97      0.98      6380\n",
      "           2       0.92      0.96      0.94      6380\n",
      "           3       0.96      0.93      0.95      6380\n",
      "\n",
      "    accuracy                           0.96     25520\n",
      "   macro avg       0.96      0.96      0.96     25520\n",
      "weighted avg       0.96      0.96      0.96     25520\n",
      "\n",
      "20:17:05 : 准确率:0.9574451410658307\n",
      "20:17:05 : store model\n",
      "20:17:06 : __________________test end__________________\n",
      "20:17:06 : train_acc:0.9171003134796238  train_epoch_loss:163.4354248046875  test_acc:0.9574451410658307  test_epoch_loss:192.0453338623047\n",
      "20:17:06 : ______________________________________________\n",
      "20:17:06 : ______________________________________________\n",
      "20:17:06 : _______________train epoch 5 end_______________\n",
      "20:17:06 : ______________________________________________\n",
      "20:17:06 : ______________________________________________\n",
      "20:17:06 : ______________________________________________\n",
      "20:17:06 : ______________________________________________\n",
      "20:17:06 : _______________train epoch6 start_______________\n",
      "20:17:06 : ______________________________________________\n",
      "20:17:06 : ______________________________________________\n",
      "20:17:06 : 正确分类的样本数：0，样本总数：0，准确率：0，ave_loss：0\n",
      "20:17:06 : ___train_one_bad_epoch___\n",
      "20:17:27 : bad_epoch_loss:46.533704712986946\tbad_acc:0.07735386769338468\n",
      "20:18:26 : 正确分类的样本数：7413，样本总数：8000，准确率：0.926625，ave_loss：0.0189783833920955669066361710429192\n",
      "20:18:26 : ___train_one_bad_epoch___\n",
      "20:18:47 : bad_epoch_loss:43.28646233677864\tbad_acc:0.0703535176758838\n",
      "20:19:46 : 正确分类的样本数：14794，样本总数：16000，准确率：0.924625，ave_loss：0.009246158413589.009217913262546062\n",
      "20:19:46 : ___train_one_bad_epoch___\n",
      "20:20:07 : bad_epoch_loss:42.65152693167329\tbad_acc:0.07350367518375919\n",
      "20:21:06 : 正确分类的样本数：22163，样本总数：24000，准确率：0.9234583333333334，ave_loss：0.00630502449348568902886015\n",
      "20:21:06 : ___train_one_bad_epoch___\n",
      "20:21:27 : bad_epoch_loss:41.11751839891076\tbad_acc:0.07175358767938397\n",
      "20:22:26 : 正确分类的样本数：29540，样本总数：32000，准确率：0.923125，ave_loss：0.004560002125799656547293297946453024\n",
      "20:22:26 : ___train_one_bad_epoch___\n",
      "20:22:48 : bad_epoch_loss:39.523788169026375\tbad_acc:0.0616030801540077\n",
      "20:23:47 : 正确分类的样本数：36909，样本总数：40000，准确率：0.922725，ave_loss：0.003812497947365045580428251810371883\n",
      "20:23:47 : ___train_one_bad_epoch___\n",
      "20:24:08 : bad_epoch_loss:36.78175466135144\tbad_acc:0.05775288764438222\n",
      "20:25:07 : 正确分类的样本数：44292，样本总数：48000，准确率：0.92275，ave_loss：0.0029834646265953780029607969336211685\n",
      "20:25:07 : ___train_one_bad_epoch___\n",
      "20:25:28 : bad_epoch_loss:37.314870182424784\tbad_acc:0.05915295764788239\n",
      "20:26:27 : 正确分类的样本数：51698，样本总数：56000，准确率：0.9231785714285714，ave_loss：0.00251719192601740363259535\n",
      "20:26:27 : ___train_one_bad_epoch___\n",
      "20:26:48 : bad_epoch_loss:34.70864325016737\tbad_acc:0.05460273013650683\n",
      "20:27:46 : __________________train end__________________13948820873，ave_loss：0.0022479214239865544\n",
      "20:27:46 : 正确分类的样本数：58858，样本总数：63800，准确率：0.922539184952978，ave_loss：0.0022520842030644417\n",
      "20:27:46 : 准确率:0.922539184952978\n",
      "20:27:46 : __________________test start__________________\n",
      "20:28:48 : 正确分类的样本数：24472，样本总数：25520，准确率：0.9589341692789969，ave_loss：0.00728080980479717256000986\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96      6380\n",
      "           1       0.99      0.97      0.98      6380\n",
      "           2       0.93      0.95      0.94      6380\n",
      "           3       0.96      0.94      0.95      6380\n",
      "\n",
      "    accuracy                           0.96     25520\n",
      "   macro avg       0.96      0.96      0.96     25520\n",
      "weighted avg       0.96      0.96      0.96     25520\n",
      "\n",
      "20:28:49 : 准确率:0.9589341692789969\n",
      "20:28:49 : store model\n",
      "20:28:49 : __________________test end__________________\n",
      "20:28:49 : train_acc:0.922539184952978  train_epoch_loss:143.6829833984375  test_acc:0.9589341692789969  test_epoch_loss:185.80625915527344\n",
      "20:28:49 : ______________________________________________\n",
      "20:28:49 : ______________________________________________\n",
      "20:28:49 : _______________train epoch 6 end_______________\n",
      "20:28:49 : ______________________________________________\n",
      "20:28:49 : ______________________________________________\n",
      "20:28:49 : ______________________________________________\n",
      "20:28:49 : ______________________________________________\n",
      "20:28:49 : _______________train epoch7 start_______________\n",
      "20:28:49 : ______________________________________________\n",
      "20:28:49 : ______________________________________________\n",
      "20:28:49 : 正确分类的样本数：0，样本总数：0，准确率：0，ave_loss：0\n",
      "20:28:49 : ___train_one_bad_epoch___\n",
      "20:29:10 : bad_epoch_loss:32.819217931479216\tbad_acc:0.0504025201260063\n",
      "20:30:09 : 正确分类的样本数：7406，样本总数：8000，准确率：0.92575，ave_loss：0.01700633764266967817057443037629128\n",
      "20:30:09 : ___train_one_bad_epoch___\n",
      "20:30:31 : bad_epoch_loss:34.24758851528168\tbad_acc:0.05530276513825691\n",
      "20:31:50 : 正确分类的样本数：14854，样本总数：16000，准确率：0.928375，ave_loss：0.00823970139026641882258442416787154\n",
      "20:31:50 : ___train_one_bad_epoch___\n",
      "20:32:34 : bad_epoch_loss:32.21457413583994\tbad_acc:0.04830241512075604\n",
      "20:33:57 : 正确分类的样本数：22276，样本总数：24000，准确率：0.9281666666666667，ave_loss：0.00549180293455719953739455\n",
      "20:33:57 : ___train_one_bad_epoch___\n",
      "20/179\tbatch_loss:0.2751275897026062\tbad_acc:0.071428571428571428\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22297/37522931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_acc_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_epoch_loss_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_22297/1213286649.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch_num)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mmin_test_epoch_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m999999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mmin_test_epoch_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_test_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22297/1213286649.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(min_test_epoch_loss, device_train, epoch_num, lmd)\u001b[0m\n\u001b[1;32m     21\u001b[0m                     ave_loss))\n\u001b[1;32m     22\u001b[0m             \u001b[0mwrite_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"___train_one_bad_epoch___\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_one_bad_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mwrite_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad_epoch_loss:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\tbad_acc:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22297/2433829328.py\u001b[0m in \u001b[0;36mtrain_one_bad_epoch\u001b[0;34m(device_train, lmd)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 将所有参数的梯度都置零\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# 误差反向传播计算参数梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moptimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# 通过梯度做一步参数更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bs0/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bs0/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bs0/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bs0/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "# cls2.load_state_dict(cls.state_dict())\n",
    "optimizer2 = optim.Adam(cls2.parameters(), lr=1e-6)\n",
    "\n",
    "train_acc_l=[]\n",
    "train_epoch_loss_l=[]\n",
    "test_acc_l=[]\n",
    "test_epoch_loss_l=[]\n",
    "train(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "cls2=fn_cls(device0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_from0(epoch_num):\n",
    "    min_test_epoch_loss=999999\n",
    "    for i in range(epoch_num):\n",
    "        min_test_epoch_loss=train_one_epoch_from0(min_test_epoch_loss,device0,i,lmd=1)\n",
    "\n",
    "    \n",
    "# train_one_epoch(device0,0)\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer2 = optim.Adam(cls2.parameters(), lr=1e-6)\n",
    "\n",
    "train_acc_l=[]\n",
    "train_epoch_loss_l=[]\n",
    "test_acc_l=[]\n",
    "test_epoch_loss_l=[]\n",
    "train_from0(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8880485893416928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90      6380\n",
      "           1       0.92      0.95      0.93      6380\n",
      "           2       0.85      0.85      0.85      6380\n",
      "           3       0.87      0.87      0.87      6380\n",
      "\n",
      "    accuracy                           0.89     25520\n",
      "   macro avg       0.89      0.89      0.89     25520\n",
      "weighted avg       0.89      0.89      0.89     25520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_all,output_all=predict_loader(device0,test1_loader,cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.9185736677115988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93      6380\n",
      "           1       0.95      0.97      0.96      6380\n",
      "           2       0.89      0.88      0.89      6380\n",
      "           3       0.89      0.92      0.90      6380\n",
      "\n",
      "    accuracy                           0.92     25520\n",
      "   macro avg       0.92      0.92      0.92     25520\n",
      "weighted avg       0.92      0.92      0.92     25520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls3=torch.load(\"../data/cls2_8_0.86577_317.2764.model\",map_location=device0)\n",
    "label_all,output_all=predict_loader(device0,test1_loader,cls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_all,output_all=predict_loader(device0,test1_loader,cls2)\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.93      0.93      0.93      6380\n",
    "#            1       0.96      0.96      0.96      6380\n",
    "#            2       0.90      0.89      0.90      6380\n",
    "#            3       0.90      0.92      0.91      6380\n",
    "\n",
    "#     accuracy                           0.92     25520\n",
    "#    macro avg       0.92      0.92      0.92     25520\n",
    "# weighted avg       0.92      0.92      0.92     25520\n",
    "\n",
    "# 23:27:24 : 准确率:0.9230015673981191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.9170846394984326\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      6380\n",
      "           1       0.94      0.97      0.95      6380\n",
      "           2       0.90      0.88      0.89      6380\n",
      "           3       0.90      0.91      0.90      6380\n",
      "\n",
      "    accuracy                           0.92     25520\n",
      "   macro avg       0.92      0.92      0.92     25520\n",
      "weighted avg       0.92      0.92      0.92     25520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls3=torch.load(\"../data/cls_bad_9_0.91708_396.36725.model\",map_location=device0)\n",
    "label_all,output_all=predict_loader(device0,test1_loader,cls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8887931034482759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90      6380\n",
      "           1       0.92      0.95      0.93      6380\n",
      "           2       0.86      0.84      0.85      6380\n",
      "           3       0.86      0.88      0.87      6380\n",
      "\n",
      "    accuracy                           0.89     25520\n",
      "   macro avg       0.89      0.89      0.89     25520\n",
      "weighted avg       0.89      0.89      0.89     25520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_all,output_all=predict_loader(device0,test2_loader,cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8913009404388714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90      6380\n",
      "           1       0.92      0.95      0.94      6380\n",
      "           2       0.86      0.84      0.85      6380\n",
      "           3       0.85      0.88      0.87      6380\n",
      "\n",
      "    accuracy                           0.89     25520\n",
      "   macro avg       0.89      0.89      0.89     25520\n",
      "weighted avg       0.89      0.89      0.89     25520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls3=torch.load(\"../data/cls2_8_0.86577_317.2764.model\",map_location=device0)\n",
    "label_all,output_all=predict_loader(device0,test2_loader,cls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8918495297805643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      6380\n",
      "           1       0.93      0.94      0.94      6380\n",
      "           2       0.87      0.85      0.86      6380\n",
      "           3       0.86      0.89      0.87      6380\n",
      "\n",
      "    accuracy                           0.89     25520\n",
      "   macro avg       0.89      0.89      0.89     25520\n",
      "weighted avg       0.89      0.89      0.89     25520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_all,output_all=predict_loader(device0,test2_loader,cls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8906347962382445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      6380\n",
      "           1       0.92      0.95      0.94      6380\n",
      "           2       0.87      0.84      0.85      6380\n",
      "           3       0.86      0.88      0.87      6380\n",
      "\n",
      "    accuracy                           0.89     25520\n",
      "   macro avg       0.89      0.89      0.89     25520\n",
      "weighted avg       0.89      0.89      0.89     25520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_all,output_all=predict_loader(device0,test2_loader,cls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "cls2=torch.load(\"../data/cls_6_0.88785_266.58456.model\",map_location=device0)\n",
    "optimizer2 = optim.Adam(cls2.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"____________________epoch:\"+str(i)+\"____________________\")\n",
    "    epoch_loss=train_one_epoch(device0,0)\n",
    "    print(epoch_loss)\n",
    "    label_all,output_all=predict_loader(device0,test1_loader,cls2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(device,s_l,cls):\n",
    "    with torch.no_grad():\n",
    "        cls.to(device)\n",
    "        cls.eval()\n",
    "        text2id = tokenizer(\n",
    "            s_l, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids=text2id[\"input_ids\"].to(device)\n",
    "        mask=text2id[\"attention_mask\"].to(device)\n",
    "        output = cls(input_ids, attention_mask=mask)\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        output1=softmax(output)\n",
    "        output2=output.argmax(dim=1)\n",
    "        return output1,output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1.6244e-02, 9.7378e-01, 5.3007e-03, 4.6791e-03],\n",
      "        [6.1406e-02, 1.4047e-03, 9.3121e-01, 5.9830e-03],\n",
      "        [7.6313e-03, 9.8954e-01, 2.2822e-03, 5.4985e-04],\n",
      "        [9.9728e-01, 8.1377e-04, 1.0767e-03, 8.2985e-04]], device='cuda:6'), tensor([1, 2, 1, 0], device='cuda:6'))\n",
      "(tensor([[1.6244e-02, 9.7378e-01, 5.3007e-03, 4.6791e-03],\n",
      "        [6.1406e-02, 1.4047e-03, 9.3121e-01, 5.9830e-03],\n",
      "        [7.6313e-03, 9.8954e-01, 2.2822e-03, 5.4985e-04],\n",
      "        [9.9728e-01, 8.1377e-04, 1.0767e-03, 8.2985e-04]], device='cuda:6'), tensor([1, 2, 1, 0], device='cuda:6'))\n"
     ]
    }
   ],
   "source": [
    "s=['Echoes Repeats Success','\"Stocks Finish Lower, Retail Sector Weighs\"','Report indicates Wannstedt out','Conference Members Back Iraqi Efforts']\n",
    "print(predict(device0,s,cls))\n",
    "print(predict(device0,s,cls2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "end=time.time()\n",
    "torch.save(cls2,\"../data/cls2_bad_\"+str(end)+\".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 0.02694982503663435\n",
    "# 0.20794415416798367\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
