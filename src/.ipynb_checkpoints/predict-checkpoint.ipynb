{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#按batch_size分\n",
    "from torch.utils.data import DataLoader,TensorDataset,Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added_token=['##char##']\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\",additional_special_tokens=added_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def text2token(text,tokenizer,max_length=100):\n",
    "    text2id = tokenizer(\n",
    "        text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids=text2id[\"input_ids\"].tolist()\n",
    "    attention_mask=text2id[\"attention_mask\"].tolist()\n",
    "    return input_ids,attention_mask\n",
    "def data2token(data_,tokenizer):\n",
    "    text=[i for i in data_['title'].values]\n",
    "    input_ids,attention_mask=text2token(text,tokenizer)\n",
    "    data_['input_ids']=input_ids\n",
    "    data_['attention_mask']=attention_mask\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.dataset = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"title\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "        pre = self.dataset.loc[idx, \"pre\"]\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        attention_mask = self.dataset.loc[idx, \"attention_mask\"]\n",
    "        sample = {\"text\": text, \"label\": label,\"pre\":pre,\"input_ids\":input_ids,\"attention_mask\":attention_mask}\n",
    "        # print(sample)\n",
    "        return sample\n",
    "    \n",
    "bad_case=pd.read_csv(\"../data/data_test1_bad.csv\")\n",
    "bad_case=data2token(bad_case,tokenizer)\n",
    "bad_loader = DataLoader(\n",
    "    SentimentDataset(bad_case), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.dataset = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"title\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        attention_mask = self.dataset.loc[idx, \"attention_mask\"]\n",
    "        sample = {\"text\": text, \"label\": label,\"input_ids\":input_ids,\"attention_mask\":attention_mask}\n",
    "        # print(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "data_test1=pd.read_csv(\"../data/data_test1.csv\")\n",
    "data_test1=data2token(data_test1,tokenizer)\n",
    "test1_loader = DataLoader(\n",
    "    SentimentDataset(data_test1), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "data_test2=pd.read_csv(\"../data/data_test2.csv\")\n",
    "data_test2=data2token(data_test2,tokenizer)\n",
    "test2_loader = DataLoader(\n",
    "    SentimentDataset(data_test2), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "data_train=pd.read_csv(\"../data/data_train.csv\")\n",
    "data_train=data2token(data_train,tokenizer)\n",
    "batch_size=16\n",
    "train_loader = DataLoader(\n",
    "    SentimentDataset(data_train), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "data_val=pd.read_csv(\"../data/data_val.csv\")\n",
    "data_val=data2token(data_val,tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    SentimentDataset(data_val), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "def predict_loader(device,test_loader,cls):\n",
    "    with torch.no_grad():\n",
    "        cls.to(device)\n",
    "        cls.eval()\n",
    "        output_all=[]\n",
    "        label_all=[]\n",
    "        for batch_idx,batch in enumerate(test_loader):\n",
    "            print(str(batch_idx)+'/'+str(len(test_loader)),end='\\r')\n",
    "            label=batch['label'].to(device)#batch size * 1\n",
    "            label_all.append(label.view(-1,1))\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device)#batch size * 100\n",
    "            attention_mask=torch.stack(batch['attention_mask']).t().to(device)#batch size * 100\n",
    "            \n",
    "            #计算输出\n",
    "            output = cls(input_ids, attention_mask=attention_mask)#batch size * 1\n",
    "            \n",
    "            #四舍五入\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            output=softmax(output)\n",
    "            output=output.argmax(dim=1)\n",
    "            output_all.append(output)\n",
    "        output_all=torch.cat(output_all,0)\n",
    "        label_all=torch.cat(label_all,0)\n",
    "\n",
    "        output_all=np.array(output_all.cpu())\n",
    "        label_all=np.array(label_all.cpu())\n",
    "        acc_score=metrics.accuracy_score(label_all,output_all)\n",
    "        print(\"准确率:\"+str(acc_score))\n",
    "        print(metrics.classification_report(label_all,output_all))\n",
    "        return label_all,output_all\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model.resize_token_embeddings(len(tokenizer))##############\n",
    "        self.model.to(device)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "#         print(outputs[0])torch.Size([8, 100, 768])\n",
    "#         print(outputs[1])torch.Size([8, 768])\n",
    "#         print(outputs[0][:,0,:])torch.Size([8, 768])\n",
    "        x = outputs[1]\n",
    "#         x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "import torch\n",
    "device0 = torch.device('cuda:7' if torch.cuda.is_available() else \"cpu\")#训练集gpu\n",
    "softmax = nn.Softmax(dim=1)\n",
    "criterion = nn.CrossEntropyLoss()#weight=weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8871081504702194\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90      6380\n",
      "           1       0.93      0.94      0.93      6380\n",
      "           2       0.87      0.82      0.85      6380\n",
      "           3       0.83      0.90      0.87      6380\n",
      "\n",
      "    accuracy                           0.89     25520\n",
      "   macro avg       0.89      0.89      0.89     25520\n",
      "weighted avg       0.89      0.89      0.89     25520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls=torch.load(\"../data/cls_bad_0_0.90823_421.46222.model\",map_location=device0)\n",
    "label_all,output_all=predict_loader(device0,test2_loader,cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8871473354231975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90      6380\n",
      "           1       0.92      0.95      0.93      6380\n",
      "           2       0.85      0.85      0.85      6380\n",
      "           3       0.85      0.88      0.87      6380\n",
      "\n",
      "    accuracy                           0.89     25520\n",
      "   macro avg       0.89      0.89      0.89     25520\n",
      "weighted avg       0.89      0.89      0.89     25520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls=torch.load(\"../data/cls_bad_1_0.92841_337.45673.model\",map_location=device0)\n",
    "label_all,output_all=predict_loader(device0,test2_loader,cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8829937304075235\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90      6380\n",
      "           1       0.93      0.94      0.93      6380\n",
      "           2       0.87      0.81      0.84      6380\n",
      "           3       0.83      0.90      0.86      6380\n",
      "\n",
      "    accuracy                           0.88     25520\n",
      "   macro avg       0.88      0.88      0.88     25520\n",
      "weighted avg       0.88      0.88      0.88     25520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls=torch.load(\"../data/cls_bad_2_0.9395_279.38382.model\",map_location=device0)\n",
    "label_all,output_all=predict_loader(device0,test2_loader,cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8813479623824452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90      6380\n",
      "           1       0.93      0.93      0.93      6380\n",
      "           2       0.85      0.83      0.84      6380\n",
      "           3       0.84      0.88      0.86      6380\n",
      "\n",
      "    accuracy                           0.88     25520\n",
      "   macro avg       0.88      0.88      0.88     25520\n",
      "weighted avg       0.88      0.88      0.88     25520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls=torch.load(\"../data/cls_bad_3_0.94937_235.75372.model\",map_location=device0)\n",
    "label_all,output_all=predict_loader(device0,test2_loader,cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8776253918495298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      6380\n",
      "           1       0.93      0.93      0.93      6380\n",
      "           2       0.83      0.84      0.84      6380\n",
      "           3       0.86      0.84      0.85      6380\n",
      "\n",
      "    accuracy                           0.88     25520\n",
      "   macro avg       0.88      0.88      0.88     25520\n",
      "weighted avg       0.88      0.88      0.88     25520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls=torch.load(\"../data/cls_bad_6_0.95893_185.80626.model\",map_location=device0)\n",
    "label_all,output_all=predict_loader(device0,test2_loader,cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
