{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bs0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "leishu=4\n",
    "device0 = torch.device('cuda:7' if torch.cuda.is_available() else \"cpu\")#训练集gpu\n",
    "device1 = torch.device('cuda:7' if torch.cuda.is_available() else \"cpu\")#测试集gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_train=pd.read_csv(\"../data/data_train.csv\")\n",
    "data_val=pd.read_csv(\"../data/data_val.csv\")\n",
    "data_test1=pd.read_csv(\"../data/data_test1.csv\")\n",
    "data_test2=pd.read_csv(\"../data/data_test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "# added_token=['##char##']\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\",additional_special_tokens=added_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def text2token(text,tokenizer,max_length=100):\n",
    "    text2id = tokenizer(\n",
    "        text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids=text2id[\"input_ids\"].tolist()\n",
    "    attention_mask=text2id[\"attention_mask\"].tolist()\n",
    "    return input_ids,attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2token(data_,tokenizer):\n",
    "    text=[i for i in data_['title'].values]\n",
    "    input_ids,attention_mask=text2token(text,tokenizer)\n",
    "    data_['input_ids']=input_ids\n",
    "    data_['attention_mask']=attention_mask\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train=data2token(data_train,tokenizer)\n",
    "data_val=data2token(data_val,tokenizer)\n",
    "data_test1=data2token(data_test1,tokenizer)\n",
    "data_test2=data2token(data_test2,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.dataset = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"title\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        attention_mask = self.dataset.loc[idx, \"attention_mask\"]\n",
    "        sample = {\"text\": text, \"label\": label,\"input_ids\":input_ids,\"attention_mask\":attention_mask}\n",
    "        # print(sample)\n",
    "        return sample\n",
    "    \n",
    "#按batch_size分\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "batch_size=16\n",
    "train_loader = DataLoader(\n",
    "    SentimentDataset(data_train), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    SentimentDataset(data_val), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "test1_loader = DataLoader(\n",
    "    SentimentDataset(data_test1), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "test2_loader = DataLoader(\n",
    "    SentimentDataset(data_test2), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model.resize_token_embeddings(len(tokenizer))##############\n",
    "        self.model.to(device)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(768, leishu)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "#         print(outputs[0])torch.Size([8, 100, 768])\n",
    "#         print(outputs[1])torch.Size([8, 768])\n",
    "#         print(outputs[0][:,0,:])torch.Size([8, 768])\n",
    "        x = outputs[1]\n",
    "#         x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "# cls = fn_cls(device0)\n",
    "# optimizer = optim.Adam(cls.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "criterion = nn.CrossEntropyLoss()#weight=weight\n",
    "### 我的类别数为12\n",
    "# criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([0.1,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])).float() ,\n",
    "#                                 size_average=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SupConLoss()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features,labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device=device0\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        if labels.shape[0] != batch_size:\n",
    "            raise ValueError('Num of labels does not match num of features')\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        \n",
    "        # compute mask_contrast\n",
    "        mask_contrast=1-mask\n",
    "\n",
    "        # compute mask_same\n",
    "        diag = torch.diag(mask)\n",
    "        a_diag = torch.diag_embed(diag)\n",
    "        mask_same= mask - a_diag\n",
    "\n",
    "        \n",
    "        # compute logits\n",
    "        logits = torch.div(torch.matmul(features, features.T),0.3)\n",
    "#         diagl = torch.diag(logits)\n",
    "#         logits=logits-diagl\n",
    "        \n",
    "        exp_logits=torch.exp(logits)\n",
    "#         b = torch.zeros(batch_size, batch_size).to(device)\n",
    "#         exp_logits=torch.where(exp_logits !=1, exp_logits, b)\n",
    "        \n",
    "    \n",
    "        # compute logits_contrast_sum_exp logits_same mask_same_sum\n",
    "        logits_contrast_exp = exp_logits * mask_contrast\n",
    "        logits_contrast_sum_exp=logits_contrast_exp.sum(1, keepdim=True)\n",
    "\n",
    "        logits_same=logits * mask_same\n",
    "        \n",
    "        mask_same_sum=mask_same.sum(1,keepdim=True)\n",
    "        \n",
    "        # compute mean_log_prob_pos\n",
    "        mean_log_prob_pos=(logits_same-torch.log(logits_contrast_sum_exp)).sum(1,keepdim=True)/mask_same_sum#某类只有一个mask_same_sum为0\n",
    "        \n",
    "        # compute loss\n",
    "#         loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss=-mean_log_prob_pos/100\n",
    "        zero = torch.zeros_like(loss)\n",
    "#         print(loss)\n",
    "#         loss=torch.where(loss > -99999, loss, zero)\n",
    "#         loss=torch.where(loss < 99999, loss, zero)\n",
    "#         print(loss)\n",
    "        return loss.mean()\n",
    "        \n",
    "#         sum_=torch.tensor([0.0]).to(device)\n",
    "#         for i in loss:\n",
    "#             if not math.isinf(i) and not math.isnan(i):\n",
    "#                 sum_+=i\n",
    "#         return sum_\n",
    "\n",
    "        \n",
    "        \n",
    "criterion_ct= SupConLoss()\n",
    "criterion_ct.to(device0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2]) tensor(-inf, device='cuda:7') \n",
      "\n",
      "\n",
      "tensor([2, 2, 2, 1]) tensor(-inf, device='cuda:7') \n",
      "\n",
      "\n",
      "tensor([2, 2, 1, 2]) tensor(inf, device='cuda:7') \n",
      "\n",
      "\n",
      "tensor([2, 1, 2, 2]) tensor(inf, device='cuda:7') \n",
      "\n",
      "\n",
      "tensor([1, 2, 2, 2]) tensor(inf, device='cuda:7') \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X = torch.tensor([[[0.2,0.2,0.6],[0.2,0.2,0.6]],[[0.1,0.1,0.8],[0.1,0.1,0.8]]])\n",
    "# X = torch.tensor([[0.1,0.1,0.8],[0.2,0.2,0.6],[0.3,0.3,0.4],[0.4,0.4,0.2]])\n",
    "X = torch.tensor([[-1.0,-1.0,2.0],[-1.0,-1.0,2.0],[-1.0,-1.0,2.0],[-1.0,2.0,-1.0]])\n",
    "y = torch.tensor([2,2,2,2])\n",
    "print(y,criterion_ct(X.to(device0),y.to(device0)),'\\n\\n')\n",
    "y = torch.tensor([2,2,2,1])\n",
    "print(y,criterion_ct(X.to(device0),y.to(device0)),'\\n\\n')\n",
    "y = torch.tensor([2,2,1,2])\n",
    "print(y,criterion_ct(X.to(device0),y.to(device0)),'\\n\\n')\n",
    "y = torch.tensor([2,1,2,2])\n",
    "print(y,criterion_ct(X.to(device0),y.to(device0)),'\\n\\n')\n",
    "y = torch.tensor([1,2,2,2])\n",
    "print(y,criterion_ct(X.to(device0),y.to(device0)),'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义日志（data文件夹下，同级目录新建一个data文件夹）\n",
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "tz = pytz.timezone('Asia/Shanghai')\n",
    "def write_log(w):\n",
    "    file_name = '../data/' + datetime.date.today().strftime('%m%d') + \"_{}.log\".format(\"bert_base-ct\")\n",
    "    t0 = datetime.datetime.now(tz).strftime('%H:%M:%S')\n",
    "    info = \"{} : {}\".format(t0, w)\n",
    "    print(info)\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(info + '\\n')\n",
    "# write_log('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(device_test):\n",
    "    cls.to(device_test)\n",
    "    cls.eval()\n",
    "\n",
    "    epoch_loss=0\n",
    "    total=0\n",
    "    correct=0\n",
    "    output_all=[]\n",
    "    label_all=[]\n",
    "    for batch_idx,batch in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "#             print(batch['label'])\n",
    "            label=batch['label'].to(device_test)#batch size * 1\n",
    "            label_all.append(label.view(-1,1))\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device_test)#batch size * 100\n",
    "            attention_mask=torch.stack(batch['attention_mask']).t().to(device_test)#batch size * 100\n",
    "            \n",
    "            #计算输出\n",
    "            output = cls(input_ids, attention_mask=attention_mask)#batch size * 1\n",
    "            total+=len(output)\n",
    "            \n",
    "            #计算loss\n",
    "            \n",
    "#             print(output,label)\n",
    "            loss = criterion(output, label)\n",
    "            epoch_loss+=loss\n",
    "            ave_loss=epoch_loss/total\n",
    "            \n",
    "            #四舍五入\n",
    "            output=softmax(output)\n",
    "            output=output.argmax(dim=1)\n",
    "            output_all.append(output)\n",
    "            \n",
    "            #计算准确率\n",
    "            add_correct=(output== label).sum().item()\n",
    "            correct+=add_correct\n",
    "            acc=correct/total\n",
    "            \n",
    "            if batch_idx%5==0:\n",
    "                print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    batch_idx, len(test_loader),100.*batch_idx/len(test_loader), \n",
    "                    correct, total,acc,\n",
    "                    ave_loss\n",
    "                    ),end= \"\\r\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    #结束：\n",
    "    write_log('正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    correct, total,acc,\n",
    "                    ave_loss))\n",
    "    \n",
    "#     can't convert cuda:5 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
    "    output_all=torch.cat(output_all,0)\n",
    "    label_all=torch.cat(label_all,0)\n",
    "    \n",
    "    output_all=np.array(output_all.cpu())\n",
    "    label_all=np.array(label_all.cpu())\n",
    "    acc_score=metrics.accuracy_score(label_all,output_all)\n",
    "    print(metrics.classification_report(label_all,output_all))\n",
    "    write_log(\"准确率:\"+str(acc_score))\n",
    "    \n",
    "    return acc,epoch_loss.item()\n",
    "\n",
    "# test(device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(min_test_epoch_loss,device_train,epoch_num):\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"_______________train epoch\"+str(epoch_num)+\" start_______________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    cls.to(device_train)\n",
    "    cls.train()\n",
    "\n",
    "    epoch_loss=0\n",
    "    total=0\n",
    "    correct=0\n",
    "    acc=0\n",
    "    ave_loss=0\n",
    "    output_all=[]\n",
    "    label_all=[]\n",
    "    for batch_idx,batch in enumerate(train_loader):\n",
    "        label=batch['label'].to(device_train)#batch size * 1\n",
    "        label_all.append(label.view(-1,1))\n",
    "        input_ids=torch.stack(batch['input_ids']).t().to(device_train)#batch size * 100\n",
    "        attention_mask=torch.stack(batch['attention_mask']).t().to(device_train)#batch size * 100\n",
    "\n",
    "        #计算输出\n",
    "        output = cls(input_ids, attention_mask=attention_mask)#batch size * 1\n",
    "\n",
    "        #计算loss\n",
    "        loss = criterion(output, label)\n",
    "        loss_ct = criterion_ct(output, label)\n",
    "        if math.isinf(loss_ct) or math.isnan(loss_ct):\n",
    "            loss_all=loss\n",
    "        else:\n",
    "            loss_all = loss+loss_ct\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_all.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            #四舍五入\n",
    "            output=softmax(output)\n",
    "#             for i in range(len(output)):\n",
    "#                 print(output[i],'\\t',label[i])\n",
    "                \n",
    "            output=output.argmax(dim=1)\n",
    "            output_all.append(output)\n",
    "            total+=len(output)\n",
    "            \n",
    "            #epoch_loss\n",
    "            epoch_loss+=loss_all############\n",
    "            ave_loss=epoch_loss/total\n",
    "            \n",
    "            #计算准确率\n",
    "            add_correct=(output== label).sum().item()\n",
    "            correct+=add_correct\n",
    "            acc=correct/total\n",
    "            \n",
    "            if batch_idx%5==0:\n",
    "                print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    batch_idx, len(train_loader),100.*batch_idx/len(train_loader), \n",
    "                    correct, total,acc,\n",
    "                    ave_loss\n",
    "                    ),end= \"\\r\")\n",
    "            \n",
    "#             print('batch:',batch_idx,'\\tloss:',loss.item(),'\\tloss_ct:',loss_ct.item(),'\\tloss_all:',loss_all.item(),'\\t准确率:',acc,'\\tave_loss:',ave_loss)\n",
    "            \n",
    "            \n",
    "    #结束：\n",
    "    \n",
    "#     can't convert cuda:5 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
    "    with torch.no_grad():\n",
    "        output_all=torch.cat(output_all,0)\n",
    "        label_all=torch.cat(label_all,0)\n",
    "\n",
    "        output_all=np.array(output_all.cpu())\n",
    "        label_all=np.array(label_all.cpu())\n",
    "        acc_score=metrics.accuracy_score(label_all,output_all)\n",
    "        \n",
    "#     print(metrics.classification_report(label_all,output_all))\n",
    "    write_log('__________________train end__________________')\n",
    "    write_log('正确分类的样本数：{}，样本总数：{}，准确率：{}，ave_loss：{}'.format(\n",
    "                    correct, total,acc,\n",
    "                    ave_loss))\n",
    "    \n",
    "    write_log(\"准确率:\"+str(acc_score))\n",
    "    \n",
    "    write_log('__________________test start__________________')\n",
    "    test_acc,test_epoch_loss=test(device1)\n",
    "    if min_test_epoch_loss>test_epoch_loss:\n",
    "        min_test_epoch_loss=test_epoch_loss\n",
    "        write_log(\"store model\")\n",
    "        end = time.time()\n",
    "        torch.save(cls,\"../data/cls_\"+str(round(test_acc,5))+'_'+str(round(test_epoch_loss,5))+\".model\")\n",
    "    \n",
    "    write_log('train_acc:'+str(acc)+'  train_epoch_loss:'+str(epoch_loss.item())+'  test_acc:'+str(test_acc)+'  test_epoch_loss:'+str(test_epoch_loss))\n",
    "    write_log('__________________test end__________________')\n",
    "    \n",
    "    train_acc_l.append(acc)\n",
    "    train_epoch_loss_l.append(epoch_loss.item())\n",
    "    test_acc_l.append(test_acc)\n",
    "    test_epoch_loss_l.append(test_epoch_loss)\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"_______________train epoch \"+str(epoch_num)+\" end_______________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    write_log(\"______________________________________________\")\n",
    "    return min_test_epoch_loss\n",
    "    \n",
    "\n",
    "    \n",
    "def train(epoch_num):\n",
    "    min_test_epoch_loss=999999\n",
    "    for i in range(epoch_num):\n",
    "        min_test_epoch_loss=train_one_epoch(min_test_epoch_loss,device0,i)\n",
    "\n",
    "    \n",
    "# train_one_epoch(device0,0)\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:15:07 : ______________________________________________\n",
      "15:15:07 : ______________________________________________\n",
      "15:15:07 : _______________train epoch0 start_______________\n",
      "15:15:07 : ______________________________________________\n",
      "15:15:07 : ______________________________________________\n",
      "15:23:07 : __________________train end__________________2298043151，ave_loss：0.0545180588960647636\n",
      "15:23:07 : 正确分类的样本数：49362，样本总数：63800，准确率：0.7736990595611285，ave_loss：0.054520100355148315\n",
      "15:23:07 : 准确率:0.7736990595611285\n",
      "15:23:07 : __________________test start__________________\n",
      "15:23:39 : 正确分类的样本数：10818，样本总数：12760，准确率：0.8478056426332289，ave_loss：0.03253014758229256887253\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.82      0.87      3190\n",
      "           1       0.89      0.90      0.90      3190\n",
      "           2       0.79      0.80      0.80      3190\n",
      "           3       0.80      0.87      0.83      3190\n",
      "\n",
      "    accuracy                           0.85     12760\n",
      "   macro avg       0.85      0.85      0.85     12760\n",
      "weighted avg       0.85      0.85      0.85     12760\n",
      "\n",
      "15:23:39 : 准确率:0.8478056426332289\n",
      "15:23:39 : store model\n",
      "15:23:39 : train_acc:0.7736990595611285  train_epoch_loss:3478.382568359375  test_acc:0.8478056426332289  test_epoch_loss:415.0846862792969\n",
      "15:23:39 : __________________test end__________________\n",
      "15:23:39 : ______________________________________________\n",
      "15:23:39 : ______________________________________________\n",
      "15:23:39 : _______________train epoch 0 end_______________\n",
      "15:23:39 : ______________________________________________\n",
      "15:23:39 : ______________________________________________\n",
      "15:23:39 : ______________________________________________\n",
      "15:23:39 : ______________________________________________\n",
      "15:23:39 : _______________train epoch1 start_______________\n",
      "15:23:39 : ______________________________________________\n",
      "15:23:39 : ______________________________________________\n",
      "15:31:53 : __________________train end__________________34269944807，ave_loss：0.039616975933313375\n",
      "15:31:53 : 正确分类的样本数：53942，样本总数：63800，准确率：0.8454858934169279，ave_loss：0.03961151838302612\n",
      "15:31:53 : 准确率:0.8454858934169279\n",
      "15:31:53 : __________________test start__________________\n",
      "15:32:25 : 正确分类的样本数：10937，样本总数：12760，准确率：0.8571316614420063，ave_loss：0.02958005666732788522457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.87      3190\n",
      "           1       0.91      0.90      0.91      3190\n",
      "           2       0.76      0.86      0.81      3190\n",
      "           3       0.83      0.85      0.84      3190\n",
      "\n",
      "    accuracy                           0.86     12760\n",
      "   macro avg       0.86      0.86      0.86     12760\n",
      "weighted avg       0.86      0.86      0.86     12760\n",
      "\n",
      "15:32:25 : 准确率:0.8571316614420063\n",
      "15:32:25 : store model\n",
      "15:32:25 : train_acc:0.8454858934169279  train_epoch_loss:2527.21484375  test_acc:0.8571316614420063  test_epoch_loss:377.4415283203125\n",
      "15:32:25 : __________________test end__________________\n",
      "15:32:25 : ______________________________________________\n",
      "15:32:25 : ______________________________________________\n",
      "15:32:25 : _______________train epoch 1 end_______________\n",
      "15:32:25 : ______________________________________________\n",
      "15:32:25 : ______________________________________________\n",
      "15:32:25 : ______________________________________________\n",
      "15:32:25 : ______________________________________________\n",
      "15:32:25 : _______________train epoch2 start_______________\n",
      "15:32:25 : ______________________________________________\n",
      "15:32:25 : ______________________________________________\n",
      "15:40:26 : __________________train end__________________6181635725，ave_loss：0.0313242487609386444\n",
      "15:40:26 : 正确分类的样本数：54981，样本总数：63800，准确率：0.8617711598746082，ave_loss：0.03132377937436104\n",
      "15:40:26 : 准确率:0.8617711598746082\n",
      "15:40:26 : __________________test start__________________\n",
      "15:40:57 : 正确分类的样本数：11084，样本总数：12760，准确率：0.8686520376175548，ave_loss：0.025915892794728287026863\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88      3190\n",
      "           1       0.91      0.92      0.92      3190\n",
      "           2       0.81      0.85      0.83      3190\n",
      "           3       0.84      0.86      0.85      3190\n",
      "\n",
      "    accuracy                           0.87     12760\n",
      "   macro avg       0.87      0.87      0.87     12760\n",
      "weighted avg       0.87      0.87      0.87     12760\n",
      "\n",
      "15:40:57 : 准确率:0.8686520376175548\n",
      "15:40:57 : store model\n",
      "15:40:58 : train_acc:0.8617711598746082  train_epoch_loss:1998.457275390625  test_acc:0.8686520376175548  test_epoch_loss:330.6867980957031\n",
      "15:40:58 : __________________test end__________________\n",
      "15:40:58 : ______________________________________________\n",
      "15:40:58 : ______________________________________________\n",
      "15:40:58 : _______________train epoch 2 end_______________\n",
      "15:40:58 : ______________________________________________\n",
      "15:40:58 : ______________________________________________\n",
      "15:40:58 : ______________________________________________\n",
      "15:40:58 : ______________________________________________\n",
      "15:40:58 : _______________train epoch3 start_______________\n",
      "15:40:58 : ______________________________________________\n",
      "15:40:58 : ______________________________________________\n",
      "15:48:58 : __________________train end__________________12042147516，ave_loss：0.027249628677964213\n",
      "15:48:58 : 正确分类的样本数：55791，样本总数：63800，准确率：0.8744670846394984，ave_loss：0.02724805660545826\n",
      "15:48:58 : 准确率:0.8744670846394984\n",
      "15:48:58 : __________________test start__________________\n",
      "15:49:30 : 正确分类的样本数：11147，样本总数：12760，准确率：0.87358934169279，ave_loss：0.02442995086312294949756622\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89      3190\n",
      "           1       0.91      0.94      0.92      3190\n",
      "           2       0.81      0.86      0.83      3190\n",
      "           3       0.85      0.86      0.86      3190\n",
      "\n",
      "    accuracy                           0.87     12760\n",
      "   macro avg       0.88      0.87      0.87     12760\n",
      "weighted avg       0.88      0.87      0.87     12760\n",
      "\n",
      "15:49:30 : 准确率:0.87358934169279\n",
      "15:49:30 : store model\n",
      "15:49:31 : train_acc:0.8744670846394984  train_epoch_loss:1738.426025390625  test_acc:0.87358934169279  test_epoch_loss:311.7261657714844\n",
      "15:49:31 : __________________test end__________________\n",
      "15:49:31 : ______________________________________________\n",
      "15:49:31 : ______________________________________________\n",
      "15:49:31 : _______________train epoch 3 end_______________\n",
      "15:49:31 : ______________________________________________\n",
      "15:49:31 : ______________________________________________\n",
      "15:49:31 : ______________________________________________\n",
      "15:49:31 : ______________________________________________\n",
      "15:49:31 : _______________train epoch4 start_______________\n",
      "15:49:31 : ______________________________________________\n",
      "15:49:31 : ______________________________________________\n",
      "15:57:30 : __________________train end__________________89914701455，ave_loss：0.025054380297660828\n",
      "15:57:30 : 正确分类的样本数：56225，样本总数：63800，准确率：0.8812695924764891，ave_loss：0.025064023211598396\n",
      "15:57:30 : 准确率:0.8812695924764891\n",
      "15:57:30 : __________________test start__________________\n",
      "15:58:02 : 正确分类的样本数：11198，样本总数：12760，准确率：0.8775862068965518，ave_loss：0.023790875449776654930115\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.89      3190\n",
      "           1       0.90      0.94      0.92      3190\n",
      "           2       0.83      0.85      0.84      3190\n",
      "           3       0.86      0.86      0.86      3190\n",
      "\n",
      "    accuracy                           0.88     12760\n",
      "   macro avg       0.88      0.88      0.88     12760\n",
      "weighted avg       0.88      0.88      0.88     12760\n",
      "\n",
      "15:58:02 : 准确率:0.8775862068965518\n",
      "15:58:02 : store model\n",
      "15:58:02 : train_acc:0.8812695924764891  train_epoch_loss:1599.084716796875  test_acc:0.8775862068965518  test_epoch_loss:303.5715637207031\n",
      "15:58:02 : __________________test end__________________\n",
      "15:58:02 : ______________________________________________\n",
      "15:58:02 : ______________________________________________\n",
      "15:58:02 : _______________train epoch 4 end_______________\n",
      "15:58:02 : ______________________________________________\n",
      "15:58:02 : ______________________________________________\n",
      "15:58:02 : ______________________________________________\n",
      "15:58:02 : ______________________________________________\n",
      "15:58:02 : _______________train epoch5 start_______________\n",
      "15:58:02 : ______________________________________________\n",
      "15:58:02 : ______________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:06:01 : __________________train end__________________85348720521，ave_loss：0.023623311892151833\n",
      "16:06:01 : 正确分类的样本数：56698，样本总数：63800，准确率：0.8886833855799373，ave_loss：0.023623796179890633\n",
      "16:06:01 : 准确率:0.8886833855799373\n",
      "16:06:01 : __________________test start__________________\n",
      "16:06:32 : 正确分类的样本数：11234，样本总数：12760，准确率：0.8804075235109717，ave_loss：0.023286866024136543238914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.89      3190\n",
      "           1       0.91      0.94      0.92      3190\n",
      "           2       0.84      0.85      0.84      3190\n",
      "           3       0.86      0.86      0.86      3190\n",
      "\n",
      "    accuracy                           0.88     12760\n",
      "   macro avg       0.88      0.88      0.88     12760\n",
      "weighted avg       0.88      0.88      0.88     12760\n",
      "\n",
      "16:06:32 : 准确率:0.8804075235109717\n",
      "16:06:32 : store model\n",
      "16:06:33 : train_acc:0.8886833855799373  train_epoch_loss:1507.1982421875  test_acc:0.8804075235109717  test_epoch_loss:297.1404113769531\n",
      "16:06:33 : __________________test end__________________\n",
      "16:06:33 : ______________________________________________\n",
      "16:06:33 : ______________________________________________\n",
      "16:06:33 : _______________train epoch 5 end_______________\n",
      "16:06:33 : ______________________________________________\n",
      "16:06:33 : ______________________________________________\n",
      "16:06:33 : ______________________________________________\n",
      "16:06:33 : ______________________________________________\n",
      "16:06:33 : _______________train epoch6 start_______________\n",
      "16:06:33 : ______________________________________________\n",
      "16:06:33 : ______________________________________________\n",
      "16:14:32 : __________________train end__________________10386352232，ave_loss：0.021688150241971016\n",
      "16:14:32 : 正确分类的样本数：57166，样本总数：63800，准确率：0.8960188087774295，ave_loss：0.021690620109438896\n",
      "16:14:32 : 准确率:0.8960188087774295\n",
      "16:14:32 : __________________test start__________________\n",
      "16:15:03 : 正确分类的样本数：11255，样本总数：12760，准确率：0.8820532915360502，ave_loss：0.022800082340836525328827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90      3190\n",
      "           1       0.92      0.93      0.92      3190\n",
      "           2       0.85      0.84      0.84      3190\n",
      "           3       0.85      0.88      0.86      3190\n",
      "\n",
      "    accuracy                           0.88     12760\n",
      "   macro avg       0.88      0.88      0.88     12760\n",
      "weighted avg       0.88      0.88      0.88     12760\n",
      "\n",
      "16:15:03 : 准确率:0.8820532915360502\n",
      "16:15:03 : store model\n",
      "16:15:04 : train_acc:0.8960188087774295  train_epoch_loss:1383.861572265625  test_acc:0.8820532915360502  test_epoch_loss:290.9290466308594\n",
      "16:15:04 : __________________test end__________________\n",
      "16:15:04 : ______________________________________________\n",
      "16:15:04 : ______________________________________________\n",
      "16:15:04 : _______________train epoch 6 end_______________\n",
      "16:15:04 : ______________________________________________\n",
      "16:15:04 : ______________________________________________\n",
      "16:15:04 : ______________________________________________\n",
      "16:15:04 : ______________________________________________\n",
      "16:15:04 : _______________train epoch7 start_______________\n",
      "16:15:04 : ______________________________________________\n",
      "16:15:04 : ______________________________________________\n",
      "16:23:03 : __________________train end__________________17310587054，ave_loss：0.020585851743817337\n",
      "16:23:03 : 正确分类的样本数：57396，样本总数：63800，准确率：0.8996238244514106，ave_loss：0.02058379165828228\n",
      "16:23:03 : 准确率:0.8996238244514106\n",
      "16:23:03 : __________________test start__________________\n",
      "16:23:34 : 正确分类的样本数：11296，样本总数：12760，准确率：0.8852664576802508，ave_loss：0.022563695907592773926125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90      3190\n",
      "           1       0.91      0.94      0.93      3190\n",
      "           2       0.84      0.86      0.85      3190\n",
      "           3       0.87      0.87      0.87      3190\n",
      "\n",
      "    accuracy                           0.89     12760\n",
      "   macro avg       0.89      0.89      0.89     12760\n",
      "weighted avg       0.89      0.89      0.89     12760\n",
      "\n",
      "16:23:34 : 准确率:0.8852664576802508\n",
      "16:23:34 : store model\n",
      "16:23:35 : train_acc:0.8996238244514106  train_epoch_loss:1313.2459716796875  test_acc:0.8852664576802508  test_epoch_loss:287.9127502441406\n",
      "16:23:35 : __________________test end__________________\n",
      "16:23:35 : ______________________________________________\n",
      "16:23:35 : ______________________________________________\n",
      "16:23:35 : _______________train epoch 7 end_______________\n",
      "16:23:35 : ______________________________________________\n",
      "16:23:35 : ______________________________________________\n",
      "16:23:35 : ______________________________________________\n",
      "16:23:35 : ______________________________________________\n",
      "16:23:35 : _______________train epoch8 start_______________\n",
      "16:23:35 : ______________________________________________\n",
      "16:23:35 : ______________________________________________\n",
      "16:31:34 : __________________train end__________________19267436026，ave_loss：0.019583784043788914\n",
      "16:31:34 : 正确分类的样本数：57760，样本总数：63800，准确率：0.9053291536050156，ave_loss：0.01959075778722763\n",
      "16:31:34 : 准确率:0.9053291536050156\n",
      "16:31:34 : __________________test start__________________\n",
      "16:32:05 : 正确分类的样本数：11333，样本总数：12760，准确率：0.888166144200627，ave_loss：0.0220106840133667139065742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90      3190\n",
      "           1       0.92      0.95      0.93      3190\n",
      "           2       0.86      0.85      0.85      3190\n",
      "           3       0.86      0.88      0.87      3190\n",
      "\n",
      "    accuracy                           0.89     12760\n",
      "   macro avg       0.89      0.89      0.89     12760\n",
      "weighted avg       0.89      0.89      0.89     12760\n",
      "\n",
      "16:32:05 : 准确率:0.888166144200627\n",
      "16:32:05 : store model\n",
      "16:32:06 : train_acc:0.9053291536050156  train_epoch_loss:1249.890380859375  test_acc:0.888166144200627  test_epoch_loss:280.8563232421875\n",
      "16:32:06 : __________________test end__________________\n",
      "16:32:06 : ______________________________________________\n",
      "16:32:06 : ______________________________________________\n",
      "16:32:06 : _______________train epoch 8 end_______________\n",
      "16:32:06 : ______________________________________________\n",
      "16:32:06 : ______________________________________________\n",
      "16:32:06 : ______________________________________________\n",
      "16:32:06 : ______________________________________________\n",
      "16:32:06 : _______________train epoch9 start_______________\n",
      "16:32:06 : ______________________________________________\n",
      "16:32:06 : ______________________________________________\n",
      "16:40:05 : __________________train end__________________60461615655，ave_loss：0.018082769587635994\n",
      "16:40:05 : 正确分类的样本数：58099，样本总数：63800，准确率：0.9106426332288401，ave_loss：0.018089041113853455\n",
      "16:40:05 : 准确率:0.9106426332288401\n",
      "16:40:05 : __________________test start__________________\n",
      "16:40:36 : 正确分类的样本数：11336，样本总数：12760，准确率：0.8884012539184953，ave_loss：0.021563179790973663105507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90      3190\n",
      "           1       0.92      0.94      0.93      3190\n",
      "           2       0.84      0.87      0.85      3190\n",
      "           3       0.87      0.87      0.87      3190\n",
      "\n",
      "    accuracy                           0.89     12760\n",
      "   macro avg       0.89      0.89      0.89     12760\n",
      "weighted avg       0.89      0.89      0.89     12760\n",
      "\n",
      "16:40:36 : 准确率:0.8884012539184953\n",
      "16:40:36 : store model\n",
      "16:40:37 : train_acc:0.9106426332288401  train_epoch_loss:1154.080810546875  test_acc:0.8884012539184953  test_epoch_loss:275.14617919921875\n",
      "16:40:37 : __________________test end__________________\n",
      "16:40:37 : ______________________________________________\n",
      "16:40:37 : ______________________________________________\n",
      "16:40:37 : _______________train epoch 9 end_______________\n",
      "16:40:37 : ______________________________________________\n",
      "16:40:37 : ______________________________________________\n"
     ]
    }
   ],
   "source": [
    "cls = fn_cls(device0)\n",
    "\n",
    "from torch import optim\n",
    "optimizer = optim.Adam(cls.parameters(), lr=1e-6)\n",
    "# test(device1)\n",
    "train_acc_l=[]\n",
    "train_epoch_loss_l=[]\n",
    "test_acc_l=[]\n",
    "test_epoch_loss_l=[]\n",
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:42:13 : ______________________________________________\n",
      "16:42:13 : ______________________________________________\n",
      "16:42:13 : _______________train epoch0 start_______________\n",
      "16:42:13 : ______________________________________________\n",
      "16:42:13 : ______________________________________________\n",
      "16:50:12 : __________________train end__________________78575012544，ave_loss：0.016516070812940598\n",
      "16:50:12 : 正确分类的样本数：58337，样本总数：63800，准确率：0.9143730407523512，ave_loss：0.016525592654943466\n",
      "16:50:12 : 准确率:0.9143730407523512\n",
      "16:50:12 : __________________test start__________________\n",
      "16:50:44 : 正确分类的样本数：11365，样本总数：12760，准确率：0.8906739811912225，ave_loss：0.02130700275301933387471\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90      3190\n",
      "           1       0.91      0.95      0.93      3190\n",
      "           2       0.86      0.85      0.86      3190\n",
      "           3       0.87      0.88      0.87      3190\n",
      "\n",
      "    accuracy                           0.89     12760\n",
      "   macro avg       0.89      0.89      0.89     12760\n",
      "weighted avg       0.89      0.89      0.89     12760\n",
      "\n",
      "16:50:44 : 准确率:0.8906739811912225\n",
      "16:50:44 : store model\n",
      "16:50:44 : train_acc:0.9143730407523512  train_epoch_loss:1054.3328857421875  test_acc:0.8906739811912225  test_epoch_loss:271.8773498535156\n",
      "16:50:44 : __________________test end__________________\n",
      "16:50:44 : ______________________________________________\n",
      "16:50:44 : ______________________________________________\n",
      "16:50:44 : _______________train epoch 0 end_______________\n",
      "16:50:44 : ______________________________________________\n",
      "16:50:44 : ______________________________________________\n",
      "16:50:44 : ______________________________________________\n",
      "16:50:44 : ______________________________________________\n",
      "16:50:44 : _______________train epoch1 start_______________\n",
      "16:50:44 : ______________________________________________\n",
      "16:50:44 : ______________________________________________\n",
      "16:58:44 : __________________train end__________________3818364275，ave_loss：0.0153552088886499445\n",
      "16:58:44 : 正确分类的样本数：58664，样本总数：63800，准确率：0.9194984326018809，ave_loss：0.015357986092567444\n",
      "16:58:44 : 准确率:0.9194984326018809\n",
      "16:58:44 : __________________test start__________________\n",
      "16:59:16 : 正确分类的样本数：11333，样本总数：12760，准确率：0.888166144200627，ave_loss：0.0213724169880151759889183\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90      3190\n",
      "           1       0.91      0.95      0.93      3190\n",
      "           2       0.84      0.87      0.85      3190\n",
      "           3       0.88      0.86      0.87      3190\n",
      "\n",
      "    accuracy                           0.89     12760\n",
      "   macro avg       0.89      0.89      0.89     12760\n",
      "weighted avg       0.89      0.89      0.89     12760\n",
      "\n",
      "16:59:16 : 准确率:0.888166144200627\n",
      "16:59:16 : train_acc:0.9194984326018809  train_epoch_loss:979.8395385742188  test_acc:0.888166144200627  test_epoch_loss:272.7120361328125\n",
      "16:59:16 : __________________test end__________________\n",
      "16:59:16 : ______________________________________________\n",
      "16:59:16 : ______________________________________________\n",
      "16:59:16 : _______________train epoch 1 end_______________\n",
      "16:59:16 : ______________________________________________\n",
      "16:59:16 : ______________________________________________\n",
      "16:59:16 : ______________________________________________\n",
      "16:59:16 : ______________________________________________\n",
      "16:59:16 : _______________train epoch2 start_______________\n",
      "16:59:16 : ______________________________________________\n",
      "16:59:16 : ______________________________________________\n",
      "17:07:15 : __________________train end__________________2513798294，ave_loss：0.0145183252170681953\n",
      "17:07:15 : 正确分类的样本数：58867，样本总数：63800，准确率：0.9226802507836991，ave_loss：0.01452120952308178\n",
      "17:07:15 : 准确率:0.9226802507836991\n",
      "17:07:15 : __________________test start__________________\n",
      "17:07:47 : 正确分类的样本数：11362，样本总数：12760，准确率：0.8904388714733542，ave_loss：0.021305344998836517427704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90      3190\n",
      "           1       0.91      0.95      0.93      3190\n",
      "           2       0.86      0.85      0.86      3190\n",
      "           3       0.87      0.87      0.87      3190\n",
      "\n",
      "    accuracy                           0.89     12760\n",
      "   macro avg       0.89      0.89      0.89     12760\n",
      "weighted avg       0.89      0.89      0.89     12760\n",
      "\n",
      "17:07:47 : 准确率:0.8904388714733542\n",
      "17:07:47 : store model\n",
      "17:07:48 : train_acc:0.9226802507836991  train_epoch_loss:926.4531860351562  test_acc:0.8904388714733542  test_epoch_loss:271.856201171875\n",
      "17:07:48 : __________________test end__________________\n",
      "17:07:48 : ______________________________________________\n",
      "17:07:48 : ______________________________________________\n",
      "17:07:48 : _______________train epoch 2 end_______________\n",
      "17:07:48 : ______________________________________________\n",
      "17:07:48 : ______________________________________________\n",
      "17:07:48 : ______________________________________________\n",
      "17:07:48 : ______________________________________________\n",
      "17:07:48 : _______________train epoch3 start_______________\n",
      "17:07:48 : ______________________________________________\n",
      "17:07:48 : ______________________________________________\n",
      "17:15:47 : __________________train end__________________75112895132，ave_loss：0.013744262047111988\n",
      "17:15:47 : 正确分类的样本数：59221，样本总数：63800，准确率：0.9282288401253919，ave_loss：0.013741612434387207\n",
      "17:15:47 : 准确率:0.9282288401253919\n",
      "17:15:47 : __________________test start__________________\n",
      "17:16:18 : 正确分类的样本数：11361，样本总数：12760，准确率：0.8903605015673981，ave_loss：0.021551692858338356406395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      3190\n",
      "           1       0.91      0.95      0.93      3190\n",
      "           2       0.87      0.84      0.85      3190\n",
      "           3       0.87      0.88      0.87      3190\n",
      "\n",
      "    accuracy                           0.89     12760\n",
      "   macro avg       0.89      0.89      0.89     12760\n",
      "weighted avg       0.89      0.89      0.89     12760\n",
      "\n",
      "17:16:18 : 准确率:0.8903605015673981\n",
      "17:16:18 : train_acc:0.9282288401253919  train_epoch_loss:876.7149047851562  test_acc:0.8903605015673981  test_epoch_loss:274.9996032714844\n",
      "17:16:18 : __________________test end__________________\n",
      "17:16:18 : ______________________________________________\n",
      "17:16:18 : ______________________________________________\n",
      "17:16:18 : _______________train epoch 3 end_______________\n",
      "17:16:18 : ______________________________________________\n",
      "17:16:18 : ______________________________________________\n",
      "17:16:18 : ______________________________________________\n",
      "17:16:18 : ______________________________________________\n",
      "17:16:18 : _______________train epoch4 start_______________\n",
      "17:16:18 : ______________________________________________\n",
      "17:16:18 : ______________________________________________\n",
      "17:24:18 : __________________train end__________________99598595083，ave_loss：0.012798746116459374\n",
      "17:24:18 : 正确分类的样本数：59480，样本总数：63800，准确率：0.9322884012539185，ave_loss：0.012804499827325344\n",
      "17:24:18 : 准确率:0.9322884012539185\n",
      "17:24:18 : __________________test start__________________\n",
      "17:24:49 : 正确分类的样本数：11378，样本总数：12760，准确率：0.8916927899686521，ave_loss：0.021373882889747620156364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90      3190\n",
      "           1       0.92      0.95      0.93      3190\n",
      "           2       0.86      0.85      0.86      3190\n",
      "           3       0.86      0.89      0.87      3190\n",
      "\n",
      "    accuracy                           0.89     12760\n",
      "   macro avg       0.89      0.89      0.89     12760\n",
      "weighted avg       0.89      0.89      0.89     12760\n",
      "\n",
      "17:24:49 : 准确率:0.8916927899686521\n",
      "17:24:49 : train_acc:0.9322884012539185  train_epoch_loss:816.9271240234375  test_acc:0.8916927899686521  test_epoch_loss:272.7307434082031\n",
      "17:24:49 : __________________test end__________________\n",
      "17:24:49 : ______________________________________________\n",
      "17:24:49 : ______________________________________________\n",
      "17:24:49 : _______________train epoch 4 end_______________\n",
      "17:24:49 : ______________________________________________\n",
      "17:24:49 : ______________________________________________\n",
      "17:24:49 : ______________________________________________\n",
      "17:24:49 : ______________________________________________\n",
      "17:24:49 : _______________train epoch5 start_______________\n",
      "17:24:49 : ______________________________________________\n",
      "17:24:49 : ______________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:32:48 : __________________train end__________________94631209232，ave_loss：0.012295782566070557\n",
      "17:32:48 : 正确分类的样本数：59621，样本总数：63800，准确率：0.9344984326018809，ave_loss：0.012295182794332504\n",
      "17:32:48 : 准确率:0.9344984326018809\n",
      "17:32:48 : __________________test start__________________\n",
      "17:33:20 : 正确分类的样本数：11368，样本总数：12760，准确率：0.8909090909090909，ave_loss：0.021843682974576959563675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90      3190\n",
      "           1       0.91      0.95      0.93      3190\n",
      "           2       0.85      0.86      0.86      3190\n",
      "           3       0.87      0.88      0.87      3190\n",
      "\n",
      "    accuracy                           0.89     12760\n",
      "   macro avg       0.89      0.89      0.89     12760\n",
      "weighted avg       0.89      0.89      0.89     12760\n",
      "\n",
      "17:33:20 : 准确率:0.8909090909090909\n",
      "17:33:20 : train_acc:0.9344984326018809  train_epoch_loss:784.4326782226562  test_acc:0.8909090909090909  test_epoch_loss:278.72540283203125\n",
      "17:33:20 : __________________test end__________________\n",
      "17:33:20 : ______________________________________________\n",
      "17:33:20 : ______________________________________________\n",
      "17:33:20 : _______________train epoch 5 end_______________\n",
      "17:33:20 : ______________________________________________\n",
      "17:33:20 : ______________________________________________\n",
      "17:33:20 : ______________________________________________\n",
      "17:33:20 : ______________________________________________\n",
      "17:33:20 : _______________train epoch6 start_______________\n",
      "17:33:20 : ______________________________________________\n",
      "17:33:20 : ______________________________________________\n",
      "17:41:20 : __________________train end__________________53537380833，ave_loss：0.011586752720177174\n",
      "17:41:20 : 正确分类的样本数：59862，样本总数：63800，准确率：0.9382758620689655，ave_loss：0.011589759029448032\n",
      "17:41:20 : 准确率:0.9382758620689655\n",
      "17:41:20 : __________________test start__________________\n",
      "17:41:51 : 正确分类的样本数：11378，样本总数：12760，准确率：0.8916927899686521，ave_loss：0.022078545764088634415627\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90      3190\n",
      "           1       0.92      0.94      0.93      3190\n",
      "           2       0.85      0.86      0.86      3190\n",
      "           3       0.87      0.87      0.87      3190\n",
      "\n",
      "    accuracy                           0.89     12760\n",
      "   macro avg       0.89      0.89      0.89     12760\n",
      "weighted avg       0.89      0.89      0.89     12760\n",
      "\n",
      "17:41:51 : 准确率:0.8916927899686521\n",
      "17:41:51 : train_acc:0.9382758620689655  train_epoch_loss:739.4266357421875  test_acc:0.8916927899686521  test_epoch_loss:281.72222900390625\n",
      "17:41:51 : __________________test end__________________\n",
      "17:41:51 : ______________________________________________\n",
      "17:41:51 : ______________________________________________\n",
      "17:41:51 : _______________train epoch 6 end_______________\n",
      "17:41:51 : ______________________________________________\n",
      "17:41:51 : ______________________________________________\n",
      "17:41:51 : ______________________________________________\n",
      "17:41:51 : ______________________________________________\n",
      "17:41:51 : _______________train epoch7 start_______________\n",
      "17:41:51 : ______________________________________________\n",
      "17:41:51 : ______________________________________________\n",
      "17:49:51 : __________________train end__________________84395383843，ave_loss：0.010865160264074802\n",
      "17:49:51 : 正确分类的样本数：60149，样本总数：63800，准确率：0.9427742946708464，ave_loss：0.0108656520023942\n",
      "17:49:51 : 准确率:0.9427742946708464\n",
      "17:49:51 : __________________test start__________________\n",
      "17:50:23 : 正确分类的样本数：11377，样本总数：12760，准确率：0.891614420062696，ave_loss：0.0227110870182514266781044\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90      3190\n",
      "           1       0.91      0.96      0.93      3190\n",
      "           2       0.85      0.86      0.86      3190\n",
      "           3       0.88      0.87      0.87      3190\n",
      "\n",
      "    accuracy                           0.89     12760\n",
      "   macro avg       0.89      0.89      0.89     12760\n",
      "weighted avg       0.89      0.89      0.89     12760\n",
      "\n",
      "17:50:23 : 准确率:0.891614420062696\n",
      "17:50:23 : train_acc:0.9427742946708464  train_epoch_loss:693.2286376953125  test_acc:0.891614420062696  test_epoch_loss:289.79345703125\n",
      "17:50:23 : __________________test end__________________\n",
      "17:50:23 : ______________________________________________\n",
      "17:50:23 : ______________________________________________\n",
      "17:50:23 : _______________train epoch 7 end_______________\n",
      "17:50:23 : ______________________________________________\n",
      "17:50:23 : ______________________________________________\n",
      "17:50:23 : ______________________________________________\n",
      "17:50:23 : ______________________________________________\n",
      "17:50:23 : _______________train epoch8 start_______________\n",
      "17:50:23 : ______________________________________________\n",
      "17:50:23 : ______________________________________________\n",
      "[1135/3988 (28%)]\t正确分类的样本数：17209，样本总数：18176，准确率：0.9467979753521126，ave_loss：0.010145269334316254\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8905/2140545206.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8905/3749368281.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch_num)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mmin_test_epoch_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m999999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mmin_test_epoch_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_test_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8905/3749368281.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(min_test_epoch_loss, device_train, epoch_num)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bs0/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bs0/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# def _plt():\n",
    "#     plt.plot([i for i in range(len(train_acc_l))], train_acc_l)\n",
    "#     plt.title('train_acc')\n",
    "#     plt.show()\n",
    "#     plt.plot([i for i in range(len(train_epoch_loss_l))], train_epoch_loss_l)\n",
    "#     plt.title('train_epoch_loss')\n",
    "#     plt.show()\n",
    "#     plt.plot([i for i in range(len(test_acc_l))], test_acc_l)\n",
    "#     plt.title('test_acc')\n",
    "#     plt.show()\n",
    "#     plt.plot([i for i in range(len(test_epoch_loss_l))], test_epoch_loss_l)\n",
    "#     plt.title('test_epoch_loss')\n",
    "#     plt.show()\n",
    "# _plt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# end = time.time()\n",
    "# torch.save(cls,\"../data/cls_\"+str(end)+\".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# cls=torch.load(\"../data/cls.model\",map_location=device0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(device,s_l,cls):\n",
    "#     with torch.no_grad():\n",
    "#         cls.to(device)\n",
    "#         cls.eval()\n",
    "#         text2id = tokenizer(\n",
    "#             s_l, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "#         )\n",
    "#         input_ids=text2id[\"input_ids\"].to(device)\n",
    "#         mask=text2id[\"attention_mask\"].to(device)\n",
    "#         output = cls(input_ids, attention_mask=mask)\n",
    "\n",
    "#         output1=softmax(output)\n",
    "#         output2=output.argmax(dim=1)\n",
    "#         return output1,output2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s=['好好好','中中中','差差差']\n",
    "# print(predict(device1,s,cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs0",
   "language": "python",
   "name": "bs0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
